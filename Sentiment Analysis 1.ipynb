{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import necessary depencencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dropout, Activation, Dense\n",
    "from keras.models import Sequential\n",
    "import model_evaluation_utils as meu\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, SpatialDropout1D\n",
    "from keras.layers import LSTM\n",
    "import xgboost as xgb\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, cross_val_predict\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "NWORKERS=15"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load normalized data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train and test datasets\n",
    "train_reviews = utils.readFromDisk('train_reviews')\n",
    "train_sentiments = utils.readFromDisk('train_sentiments')\n",
    "test_reviews = utils.readFromDisk('test_reviews')\n",
    "test_sentiments = utils.readFromDisk('test_sentiments')\n",
    "tokenized_train = utils.readFromDisk('tokenized_train')\n",
    "tokenized_test = utils.readFromDisk('tokenized_test')\n",
    "train_sentiments_encoded = utils.readFromDisk('train_sentiments_encoded')\n",
    "test_sentiments_encoded = utils.readFromDisk('test_sentiments_encoded')\n",
    "\n",
    "#glove features\n",
    "# feature engineering with GloVe model\n",
    "train_glove_features = utils.readFromDisk('train_glove_features')\n",
    "test_glove_features = utils.readFromDisk('test_glove_features')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#split train into train and vaildate sets for models that need it\n",
    "def splitTrainForValidation(train_x,train_y,val_ratio):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=val_ratio)\n",
    "    return X_train, X_val, y_train, y_val\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression (hyperparameter tuned and cached)\n",
    "# #######################################################################################################33333"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    lr=utils.readFromDisk('lr')\n",
    "except FileNotFoundError as te:\n",
    "    #lr0 = LogisticRegression(max_iter=1000,n_jobs=NWORKERS)\n",
    "    lr0 = LogisticRegression()\n",
    "\n",
    "    lr_n_jobs=[NWORKERS]\n",
    "    lr_max_iter=[1000]\n",
    "    lr_penalty = ['l2']\n",
    "    lr_C = np.logspace(0, 4, 10)\n",
    "    # Create hyperparameter options\n",
    "    lr_hyperparameters = dict(max_iter=lr_max_iter,C=lr_C, penalty=lr_penalty,n_jobs=lr_n_jobs)\n",
    "\n",
    "    # Create grid search using 5-fold cross validation\n",
    "    #had to set njobs here to nworkers here because njobs on the algo above was not triggering multiproc\n",
    "    lr_clf = GridSearchCV(lr0, lr_hyperparameters,cv=5, n_jobs=NWORKERS,scoring = 'roc_auc')\n",
    "\n",
    "    # Fit grid search\n",
    "    lr = lr_clf.fit(train_glove_features,train_sentiments)\n",
    "\n",
    "    utils.writeToDisk(lr,'lr')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    # View best hyperparameters\n",
    "    lr.best_estimator_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Predict target vector\n",
    "lr_predictions=lr.predict(test_glove_features)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression - Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_predictions,\n",
    "                                      classes=[1,0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM using sgd classifier (hyperparameter tuned and cached)\n",
    "# #######################################################################################################33333"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    svm=utils.readFromDisk('svm')\n",
    "except FileNotFoundError as te:\n",
    "    #svm0 = SGDClassifier(max_iter=500,n_jobs=NWORKERS)\n",
    "    svm0 = SGDClassifier()\n",
    "\n",
    "    svm_n_jobs=[NWORKERS]\n",
    "    svm_max_iter=[1000]\n",
    "    svm_loss= [\"hinge\"]\n",
    "    svm_alpha = [0.0001, 0.001, 0.01, 0.1]\n",
    "    svm_penalty = [\"l2\"]\n",
    "    svm_hyperparameters = dict(max_iter=lr_max_iter,loss=svm_loss, alpha=svm_alpha,penalty=svm_penalty,n_jobs=svm_n_jobs)\n",
    "\n",
    "    svm_clf = GridSearchCV(svm0, svm_hyperparameters,cv=5, n_jobs=1,scoring = 'roc_auc')\n",
    "\n",
    "    svm = svm_clf.fit(train_glove_features,train_sentiments)\n",
    "\n",
    "    utils.writeToDisk(svm,'svm')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    svm.best_estimator_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svm_predictions=svm.predict(test_glove_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVM - Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_predictions,\n",
    "                                      classes=[1,0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# XGBoost (hyperparameter tuned and cached)\n",
    "# #######################################################################################################33333"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    xg=utils.readFromDisk('xg')\n",
    "except FileNotFoundError as te:\n",
    "#    xg = xgb.XGBClassifier(n_estimators=500, max_depth=5, base_score=0.5,objective='binary:logistic', \n",
    "#                       random_state=42,nthread=16,n_jobs=NWORKERS)\n",
    "    xg0 = xgb.XGBClassifier()\n",
    "\n",
    "    xg_max_depth=[10]\n",
    "    xg_n_estimators=[200]\n",
    "    xg_learning_rate=[0.1, 0.01, 0.05]\n",
    "    xg_base_score=[0.5]\n",
    "    xg_objective=['binary:logistic']\n",
    "    xg_random_state=[42]\n",
    "    xg_n_thread=[1]\n",
    "    xg_n_jobs=[NWORKERS]\n",
    "    xg_hyperparameters = dict(max_depth=xg_max_depth,n_estimators=xg_n_estimators, \n",
    "                              learning_rate=xg_learning_rate, base_score=xg_base_score,\n",
    "                              objective=xg_objective,random_state=xg_random_state,\n",
    "                              n_thread=xg_n_thread,n_jobs=xg_n_jobs)\n",
    "\n",
    "    xg_clf = GridSearchCV(xg0, xg_hyperparameters,cv=5, n_jobs=1,scoring = 'roc_auc')\n",
    "    \n",
    "    xg=xg_clf.fit(train_glove_features, train_sentiments)\n",
    "\n",
    "    utils.writeToDisk(xg,'xg')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    xg.best_estimator_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xg_predictions= xg.predict(test_glove_features)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost - Performance\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=xg_predictions, \n",
    "                                      classes=[1,0])  \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Deep neural network architecture (DNN) (hyperparameter tuned and cached)\n",
    "# #######################################################################################################33333"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def construct_deepnn_architecture(num_input_features):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(512, activation='relu', input_shape=(num_input_features,)))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(512, activation='relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(512, activation='relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(2))\n",
    "    dnn_model.add(Activation('softmax'))\n",
    "\n",
    "    dnn_model.compile(loss='categorical_crossentropy', optimizer='adam',                 \n",
    "                      metrics=['accuracy'])\n",
    "    return dnn_model\n",
    "def build_keras_base(hidden_layers = [64, 64, 64], dropout_rate = 0, \n",
    "                     l2_penalty = 0.1, optimizer = 'adam',\n",
    "                     n_input = 100, n_class = 2):\n",
    "    \"\"\"\n",
    "    Keras Multi-layer neural network. Fixed parameters include: \n",
    "    1. activation function (PRelu)\n",
    "    2. always uses batch normalization after the activation\n",
    "    3. use adam as the optimizer\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Tunable parameters are (commonly tuned)\n",
    "    \n",
    "    hidden_layers: list\n",
    "        the number of hidden layers, and the size of each hidden layer\n",
    "    \n",
    "    dropout_rate: float 0 ~ 1\n",
    "        if bigger than 0, there will be a dropout layer\n",
    "    \n",
    "    l2_penalty: float\n",
    "        or so called l2 regularization\n",
    "    \n",
    "    optimizer: string or keras optimizer\n",
    "        method to train the network\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : \n",
    "        a keras model\n",
    "\n",
    "    Reference\n",
    "    ---------\n",
    "    https://keras.io/scikit-learn-api/\n",
    "    \"\"\"   \n",
    "    model = Sequential()   \n",
    "    for index, layers in enumerate(hidden_layers):       \n",
    "        if not index:\n",
    "            # specify the input_dim to be the number of features for the first layer\n",
    "            model.add(Dense(layers, input_dim = n_input, kernel_regularizer = l2(l2_penalty)))\n",
    "        else:\n",
    "            model.add(Dense(layers, kernel_regularizer = l2(l2_penalty)))\n",
    "        \n",
    "        # insert BatchNorm layer immediately after fully connected layers\n",
    "        # and before activation layer\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(PReLU())        \n",
    "        if dropout_rate:\n",
    "            model.add(Dropout(p = dropout_rate))\n",
    "    \n",
    "    model.add(Dense(n_class))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # the loss for binary and muti-class classification is different \n",
    "    loss = 'binary_crossentropy'\n",
    "    if n_class > 2:\n",
    "        loss = 'categorical_crossentropy'\n",
    "    \n",
    "    model.compile(loss = loss, optimizer = optimizer, metrics = ['accuracy'])   \n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DNN - Training and Prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "try:   \n",
    "    dnn=utils.readFromDisk('dnn')\n",
    "except FileNotFoundError as te:\n",
    "    #from http://ethen8181.github.io/machine-learning/keras/nn_keras_hyperparameter_tuning.html\n",
    "    dnn0 = KerasClassifier(build_fn = construct_deepnn_architecture(num_input_features=300)    )\n",
    "    # random search's parameter:\n",
    "    # specify the options and store them inside the dictionary\n",
    "    # batch size and training method can also be hyperparameters, \n",
    "    # but it is fixed\n",
    "    dnn_dropout_rate_opts  = [0, 0.2, 0.5]\n",
    "    dnn_hidden_layers_opts = [[64, 64, 64, 64], [32, 32, 32, 32, 32], [100, 100, 100]]\n",
    "    dnn_l2_penalty_opts = [0.01, 0.1, 0.5]\n",
    "    dnn_hyperparameters = dict(hidden_layers=dnn_hidden_layers_opts,dropout_rate=dnn_dropout_rate_opts,\n",
    "                               l2_penalty=dnn_l2_penalty_opts)\n",
    "\n",
    "    dnn_epochs=[5]\n",
    "    dnn_batch_size=[100]\n",
    "    dnn_validation_split=[0.1]\n",
    "    dnn_shuffle=[True]\n",
    "    dnn_workers=[NWORKERS]\n",
    "    dnn_use_multiprocessing=[True]\n",
    "    dnn_fit_parameters = dict(epochs=dnn_epochs, batch_size=dnn_batch_size,validation_split=dnn_validation_split,\n",
    "                               shuffle=dnn_shuffle, workers=dnn_workers, use_multiprocessing=dnn_use_multiprocessing)\n",
    "    \n",
    "    dnn_clf = RandomizedSearchCV(\n",
    "        dnn0, \n",
    "        param_distributions = dnn_hyperparameters,\n",
    "        scoring = 'neg_log_loss',\n",
    "        n_iter = 3, \n",
    "        cv = 5,\n",
    "        n_jobs = NWORKERS,\n",
    "        verbose = 1\n",
    "    )    \n",
    "    \n",
    "    '''   \n",
    "    \n",
    "    dnn0 = construct_deepnn_architecture(num_input_features=300)\n",
    "    k_dnn0 = KerasClassifier(dnn0, verbose=0)\n",
    "\n",
    "    dnn_epochs=[5]\n",
    "    dnn_batch_size=[100]\n",
    "    dnn_validation_split=[0.1]\n",
    "    dnn_shuffle=[True]\n",
    "    dnn_workers=[NWORKERS]\n",
    "    dnn_use_multiprocessing=[True]\n",
    "    #dnn_dropout_rate_opts  = [0, 0.2, 0.5]\n",
    "    #dnn_hidden_layers_opts = [[64, 64, 64, 64], [32, 32, 32, 32, 32], [100, 100, 100]]\n",
    "    #dnn_l2_penalty_opts = [0.01, 0.1, 0.5]\n",
    "    dnn_hyperparameters = dict(epochs=dnn_epochs, batch_size=dnn_batch_size,validation_split=dnn_validation_split,\n",
    "                               shuffle=dnn_shuffle, workers=dnn_workers, use_multiprocessing=dnn_use_multiprocessing)\n",
    "\n",
    "    #batch_size = 100\n",
    "    #dnn.fit(train_glove_features, train_sentiments_encoded, epochs=5, batch_size=batch_size, \n",
    "    #        shuffle=True, validation_split=0.1, verbose=1,workers=NWORKERS,use_multiprocessing=True)\n",
    "    dnn_clf = RandomizedSearchCV(\n",
    "        k_dnn0, \n",
    "        dnn_hyperparameters,\n",
    "        scoring = 'neg_log_loss',\n",
    "        cv = 5,\n",
    "        n_jobs = 1)\n",
    "    '''\n",
    "    \n",
    "    dnn=dnn_clf.fit(train_glove_features,train_sentiments_encoded,fit_params=dnn_fit_parameters)    \n",
    "    \n",
    "    utils.writeToDisk(dnn,'dnn')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "le = utils.readFromDisk('label_encoder')\n",
    "y_pred = dnn.predict_classes(test_glove_features)\n",
    "dnn_predictions = le.inverse_transform(y_pred) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DNN - Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=dnn_predictions, \n",
    "                                      classes=[1,0])  \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LSTM\n",
    "# #######################################################################################################33333\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# build word to index vocabulary\n",
    "token_counter = Counter([token for review in tokenized_train for token in review])\n",
    "vocab_map = {item[0]: index+1 for index, item in enumerate(dict(token_counter).items())}\n",
    "max_index = np.max(list(vocab_map.values()))\n",
    "vocab_map['PAD_INDEX'] = 0\n",
    "vocab_map['NOT_FOUND_INDEX'] = max_index+1\n",
    "vocab_size = len(vocab_map)\n",
    "# view vocabulary size and part of the vocabulary map\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Sample slice of vocabulary map:', dict(list(vocab_map.items())[10:20]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM- Encode and Pad datasets & Encode prediction class labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get max length of train corpus and initialize label encoder\n",
    "max_len = np.max([len(review) for review in tokenized_train])\n",
    "\n",
    "## Train reviews data corpus\n",
    "# Convert tokenized text reviews to numeric vectors\n",
    "train_X = [[vocab_map[token] for token in tokenized_review] for tokenized_review in tokenized_train]\n",
    "train_X = sequence.pad_sequences(train_X, maxlen=max_len) # pad \n",
    "\n",
    "## Test reviews data corpus\n",
    "# Convert tokenized text reviews to numeric vectors\n",
    "test_X = [[vocab_map[token] if vocab_map.get(token) else vocab_map['NOT_FOUND_INDEX'] \n",
    "           for token in tokenized_review] \n",
    "              for tokenized_review in tokenized_test]\n",
    "test_X = sequence.pad_sequences(test_X, maxlen=max_len)\n",
    "\n",
    "# view vector shapes\n",
    "print('Max length of train review vectors:', max_len)\n",
    "print('Train review vectors shape:', train_X.shape, ' Test review vectors shape:', test_X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM- Build, train and visualize the LSTM Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128 # dimension for dense embeddings for each token\n",
    "LSTM_DIM = 64 # total LSTM units\n",
    "\n",
    "lstm = Sequential()\n",
    "\n",
    "try:   \n",
    "    lstm=utils.readFromDisk('lstm')\n",
    "    print(lstm.summary())\n",
    "except FileNotFoundError as te:    \n",
    "    lstm.add(Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_len))\n",
    "    lstm.add(SpatialDropout1D(0.2))\n",
    "    lstm.add(LSTM(LSTM_DIM, dropout=0.2, recurrent_dropout=0.2))\n",
    "    lstm.add(Dense(2, activation=\"sigmoid\"))\n",
    "    \n",
    "    lstm.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    print(lstm.summary())\n",
    "\n",
    "    SVG(model_to_dot(lstm, show_shapes=True, show_layer_names=False, \n",
    "                     rankdir='LR').create(prog='dot', format='svg'))\n",
    "    batch_size = 100\n",
    "    lstm.fit(train_X, train_sentiments_encoded, epochs=2, batch_size=batch_size, \n",
    "              shuffle=True, validation_split=0.1, verbose=1,workers=NWORKERS,use_multiprocessing=True)\n",
    "\n",
    "    utils.writeToDisk(lstm,'lstm')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM - Predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lstm_pred_test = lstm.predict_classes(test_X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lstm_predictions = le.inverse_transform(lstm_pred_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM - Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lstm_predictions, \n",
    "                                      classes=[1,0])  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ensemble Stacking\n",
    "# #######################################################################################################33333"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf1 = KNeighborsClassifier(n_neighbors=1,n_jobs=NWORKERS)\n",
    "clf2 = RandomForestClassifier(random_state=1,n_jobs=NWORKERS)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression(penalty='l2', max_iter=500, C=1,n_jobs=NWORKERS)\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n",
    "                          meta_classifier=lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensemble Stacking - Predictions via n fold validations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf_list = [clf1, clf2, clf3, sclf]\n",
    "#preds_list=[]\n",
    "\n",
    "try:\n",
    "    clf_list=utils.readFromDisk('clf_list')\n",
    "    #preds_list=utils.readFromDisk('preds_list')\n",
    "except FileNotFoundError as te:   \n",
    "    clf_list = [clf1, clf2, clf3, sclf]\n",
    "    preds_list=[]\n",
    "    for clf in clf_list:\n",
    "        clf.fit(train_glove_features,train_sentiments)\n",
    "        #pred = cross_val_predict(clf, train_glove_features, train_sentiments, cv=2,n_jobs=NWORKERS)\n",
    "        #preds_list.append(pred)\n",
    "    \n",
    "    utils.writeToDisk(clf_list,'clf_list')\n",
    "    #utils.writeToDisk(preds_list,'preds_list')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    knn_predictions=utils.readFromDisk('knn_predictions')\n",
    "except FileNotFoundError as te:    \n",
    "    knn_predictions=clf1.predict(test_glove_features)\n",
    "    utils.writeToDisk(knn_predictions,'knn_predictions')\n",
    "try:\n",
    "    rf_predictions=utils.readFromDisk('rf_predictions')\n",
    "except FileNotFoundError as te:    \n",
    "    rf_predictions=clf2.predict(test_glove_features)\n",
    "    utils.writeToDisk(rf_predictions,'rf_predictions')\n",
    "\n",
    "try:\n",
    "    gau_predictions=utils.readFromDisk('gau_predictions')\n",
    "except FileNotFoundError as te:    \n",
    "    gau_predictions=clf3.predict(test_glove_features)\n",
    "    utils.writeToDisk(gau_predictions,'gau_predictions')\n",
    "\n",
    "try:\n",
    "    stack_predictions=utils.readFromDisk('stack_predictions')\n",
    "except FileNotFoundError as te:    \n",
    "    stack_predictions=sclf.predict(test_glove_features)\n",
    "    utils.writeToDisk(stack_predictions,'stack_predictions')\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# All Models Evaluation\n",
    "# #######################################################################################################33333"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model ROC curves"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(0).clf()\n",
    "\n",
    "color = ['blue', 'orange', 'red', 'green', 'coral',\n",
    "             'grey', 'indigo', 'gold', 'lime', 'olive',\n",
    "             'pink', 'navy', 'magenta', 'yellow', 'tomato',\n",
    "             'turquoise', 'yellowgreen', 'maroon', 'lightblue']\n",
    "mlr=[]\n",
    "msvm=[]\n",
    "mdnn=[]\n",
    "mxg=[]\n",
    "mlstm=[]\n",
    "mknn=[]\n",
    "mrf=[]\n",
    "mgau=[]\n",
    "mstack=[]\n",
    "\n",
    "def metricsAndROC(pred,metricsArray,rocTitle,colorIndex):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_sentiments, pred)\n",
    "    auc = metrics.roc_auc_score(test_sentiments, pred)\n",
    "    metricsArray.append(metrics.f1_score(test_sentiments, pred))\n",
    "    metricsArray.append(metrics.precision_score(test_sentiments, pred))\n",
    "    metricsArray.append(metrics.accuracy_score(test_sentiments, pred))\n",
    "    metricsArray.append(metrics.recall_score(test_sentiments, pred))\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr,color=color[colorIndex], label=rocTitle.format(auc))\n",
    "\n",
    "\n",
    "\n",
    "metricsAndROC(lr_predictions,mlr,'LR',0)\n",
    "metricsAndROC(svm_predictions,msvm,'SVM',1)\n",
    "metricsAndROC(dnn_predictions,mdnn,'DNN',2)\n",
    "metricsAndROC(xg_predictions,mxg,'XGBoost',3)\n",
    "metricsAndROC(lstm_predictions,mlstm,'LSTM',4)\n",
    "metricsAndROC(knn_predictions,mknn,'KNN',5)\n",
    "metricsAndROC(rf_predictions,mrf,'RFOR',6)\n",
    "metricsAndROC(gau_predictions,mgau,'GAUS',7)\n",
    "metricsAndROC(stack_predictions,mstack,'ENSEM-STACK',8)\n",
    "\n",
    "\n",
    "\n",
    "#show the roc curve now\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "# show the legend\n",
    "plt.legend(loc='center right')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## All Models Metrics comparison"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_groups = 4\n",
    "index = np.arange(n_groups)\n",
    "bar_width = .1\n",
    "\n",
    "plt.bar(index,mlr, bar_width, color=color[0], label='LR')\n",
    "\n",
    "\n",
    "z=index + bar_width\n",
    "plt.bar(z, msvm, bar_width, color=color[1],label='SVM')\n",
    "\n",
    "\n",
    "z=z+ bar_width\n",
    "plt.bar(z, mdnn, bar_width, color=color[2], label='DNN')\n",
    "\n",
    "z=z+ bar_width\n",
    "plt.bar(z,mxg , bar_width, color=color[3], label='XGB')\n",
    "\n",
    "z=z+ bar_width\n",
    "plt.bar(z,mlstm , bar_width,color=color[4], label='LSTM')\n",
    "\n",
    "\n",
    "\n",
    "#ax.set_xlabel('Metric')\n",
    "#ax.set_ylabel('Value')\n",
    "#ax.set_title('Comparison of Feature Engineering Models on Amazon Reviews')\n",
    "#ax.set_xticks(index + bar_width / 2)\n",
    "pltLabels=['F1','PRECISION','ACCURACY','RECALL']\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('Model Metrics', fontweight='bold')\n",
    "plt.xticks([r + bar_width for r in range(n_groups)], pltLabels)\n",
    " \n",
    "# Create legend & Show graphic\n",
    "plt.legend(frameon=False,ncol=3, loc='lower left')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}