{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Supervised Feature Engineeering of Reviews for Cellphone and Accessories category on Amazon "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import model_evaluation_utils as meu\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import warnings\n",
    "from sklearn.linear_model import  SGDClassifier\n",
    "from gensim.models.fasttext import FastText\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nlp = spacy.load('en_vecs', parse=False, tag=False, entity=False)\n",
    "\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "\n",
    "FILENAME_PREFIX= './data/amazon_reviews_' \n",
    "\n",
    "NWORKERS=16\n",
    "\n",
    "NUMFEATURES=100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def writeToDisk(object,name):\n",
    "    with open(FILENAME_PREFIX+name+'.pickle', \"wb\") as f:\n",
    "        pickle.dump(object, f)\n",
    "\n",
    "def readFromDisk(name):        \n",
    "    f=open(FILENAME_PREFIX+name+'.pickle', \"rb\")\n",
    "    return pickle.load(f)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load normalized data from processed file\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfdb = readFromDisk('processed')\n",
    "\n",
    "#filter rows out that have less than 20 word tokens\n",
    "#dfdb = dfdb[dfdb['Clean_Review_Tokens'].apply(lambda x: len(x) >= 20)]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample processed data loaded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Total Rows on processed dataset: ' + str(len(dfdb)))\n",
    "print('Sample of processed dataset. Notice the column named Clean_Review');\n",
    "dfdb.head(20)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split train and test data for both normalized and raw\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# take a peek at the data\n",
    "reviews = np.array(dfdb['Clean_Review'])\n",
    "reviews_raw = np.array(dfdb['reviewText'])\n",
    "sentiments = np.array(dfdb['sentiment'])\n",
    "reviews_tokens = np.array(dfdb['Clean_Review_Tokens'])\n",
    "\n",
    "cutoff=round(len(dfdb)*0.75)\n",
    "# build train and test datasets\n",
    "train_reviews = reviews[:cutoff]\n",
    "train_reviews_raw = reviews_raw[:cutoff]\n",
    "train_reviews_tokens = reviews_tokens[:cutoff]\n",
    "\n",
    "train_sentiments = sentiments[:cutoff]\n",
    "train_sentiments=train_sentiments.astype('int')\n",
    "\n",
    "test_reviews = reviews[cutoff:]\n",
    "test_reviews_raw = reviews_raw[cutoff:]\n",
    "test_reviews_tokens = reviews_tokens[cutoff:]\n",
    "\n",
    "test_sentiments = sentiments[cutoff:]\n",
    "test_sentiments=test_sentiments.astype('int')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "writeToDisk(reviews,'reviews')\n",
    "writeToDisk(reviews_raw,'reviews_raw')\n",
    "writeToDisk(sentiments,'sentiments')\n",
    "writeToDisk(reviews_tokens,'reviews_tokens')\n",
    "\n",
    "writeToDisk(train_reviews,'train_reviews')\n",
    "writeToDisk(train_reviews_raw,'train_reviews_raw')\n",
    "writeToDisk(train_reviews_tokens,'train_reviews_tokens')\n",
    "writeToDisk(train_sentiments,'train_sentiments')\n",
    "\n",
    "writeToDisk(test_reviews,'test_reviews')\n",
    "writeToDisk(test_reviews_raw,'test_reviews_raw')\n",
    "writeToDisk(test_reviews_tokens,'test_reviews_tokens')\n",
    "writeToDisk(test_sentiments,'test_sentiments')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample train data loaded, notice Cleaned Review"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Total Rows on train dataset: ' + str(len(train_reviews)))\n",
    "print('Total Rows on test dataset: ' + str(len(test_reviews)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering using BOW"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# build BOW features on train reviews\n",
    "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\n",
    "cv_train_features = cv.fit_transform(train_reviews)\n",
    "# transform test reviews into features\n",
    "cv_test_features = cv.transform(test_reviews)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM Model Training, Prediction, Performance with BOW\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svm = SGDClassifier(loss='hinge', max_iter=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svm_bow_predictions = meu.train_predict_model(classifier=svm, train_features=cv_train_features, \n",
    "                                              train_labels=train_sentiments,test_features=cv_test_features, \n",
    "                                              test_labels=test_sentiments)\n",
    "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_bow_predictions,classes=[1, 0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering using NGRAM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# build ngram features on train reviews\n",
    "cvn = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(2,2))\n",
    "cvn_train_features = cvn.fit_transform(train_reviews)\n",
    "# transform test reviews into features\n",
    "cvn_test_features = cvn.transform(test_reviews)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM Model Training, Prediction, Performance with NGRAM\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svm_ngram_predictions = meu.train_predict_model(classifier=svm, train_features=cvn_train_features, train_labels=train_sentiments,test_features=cvn_test_features, test_labels=test_sentiments)\n",
    "print('NGRAM model:> Train features shape:', cvn_train_features.shape, ' Test features shape:', cvn_test_features.shape)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_ngram_predictions,classes=[1, 0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering using TFIDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# build TFIDF features on train reviews\n",
    "tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0, ngram_range=(1,2),\n",
    "                     sublinear_tf=True)\n",
    "tv_train_features = tv.fit_transform(train_reviews)\n",
    "tv_test_features = tv.transform(test_reviews)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM Model Training, Prediction,Performance with TFIDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svm_tfidf_predictions = meu.train_predict_model(classifier=svm, \n",
    "                                                train_features=tv_train_features, train_labels=train_sentiments,\n",
    "                                                test_features=tv_test_features, test_labels=test_sentiments)\n",
    "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_tfidf_predictions,classes=[1, 0])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prediction class label encoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tokenize train reviews & encode train labels\n",
    "tn = ToktokTokenizer()\n",
    "tokenized_train_raw = [tn.tokenize(text)                  for text in train_reviews_raw]\n",
    "tokenized_test_raw = [tn.tokenize(text)                   for text in test_reviews_raw]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering with word embeddings (Word2Vec/Gensim)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# build word2vec model using gensim\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(tokenized_train_raw,workers=NWORKERS)    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def averaged_word2vec_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    \n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "        nwords = 0.\n",
    "        \n",
    "        for word in words:\n",
    "            if word in vocabulary: \n",
    "                nwords = nwords + 1.\n",
    "                feature_vector = np.add(feature_vector, model[word])\n",
    "        if nwords:\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "        return feature_vector\n",
    "\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# generate averaged word vector features from word2vec model\n",
    "avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train_raw, model=w2v_model,num_features=NUMFEATURES)\n",
    "avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test_raw, model=w2v_model,num_features=NUMFEATURES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM Model Training, Prediction, Performance with Word2Vec\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svm_wv_predictions = meu.train_predict_model(classifier=svm, \n",
    "                                                train_features=avg_wv_train_features, train_labels=train_sentiments,\n",
    "                                                test_features=avg_wv_test_features, test_labels=test_sentiments)\n",
    "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape, ' Test features shape:', avg_wv_test_features.shape)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_wv_predictions,classes=[1, 0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM Model Training, Prediction, Performance with GLoVe\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# feature engineering with GloVe model\n",
    "train_nlp = [nlp(item) for item in train_reviews_raw]\n",
    "train_glove_features = np.array([item.vector for item in train_nlp])\n",
    "writeToDisk(train_glove_features,'train_glove_features')\n",
    "\n",
    "test_nlp = [nlp(item) for item in test_reviews_raw]\n",
    "test_glove_features = np.array([item.vector for item in test_nlp])\n",
    "writeToDisk(test_glove_features,'test_glove_features')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svm_glove_predictions = meu.train_predict_model(classifier=svm, \n",
    "                                                train_features=train_glove_features, train_labels=train_sentiments,\n",
    "                                                test_features=test_glove_features, test_labels=test_sentiments)\n",
    "print('Glove model:> Train features shape:', train_glove_features.shape, ' Test features shape:', test_glove_features.shape)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_glove_predictions,classes=[1, 0])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM Model Training, Prediction, Performance with FastText (Reduced dataset due to memory constraints)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sg decides whether to use the skip-gram model (1) or CBOW (0) \n",
    "#ft_model = FastText( tokenized_train1, size = ft_num_features, window = 50,min_count = 5,\n",
    "#                     sample = 1e-3, sg = 1, iter = max_iter, workers = NWORKERS) \n",
    "ft_model = FastText( tokenized_train_raw, size = NUMFEATURES,workers = NWORKERS) \n",
    "# generate averaged word vector features from word2vec model \n",
    "train_ft_features = averaged_word2vec_vectorizer( corpus = tokenized_train_raw, num_features=NUMFEATURES,model = ft_model) \n",
    "test_ft_features = averaged_word2vec_vectorizer( corpus = tokenized_test_raw,num_features=NUMFEATURES, model = ft_model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svm_ft_predictions = meu.train_predict_model(classifier=svm, \n",
    "                                                train_features=train_ft_features, train_labels=train_sentiments,\n",
    "                                                test_features=test_ft_features, test_labels=test_sentiments)\n",
    "print('FastText:> Train features shape:', train_ft_features.shape, ' Test features shape:', test_ft_features.shape)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_ft_predictions,classes=[1, 0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ROC curves for SVM applied to various feature engineering methods- BOW, NGRAM, TFIDF, GLoVe, FastText"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(0).clf()\n",
    "\n",
    "color = ['blue', 'orange', 'red', 'green', 'coral',\n",
    "             'grey', 'indigo', 'gold', 'lime', 'olive',\n",
    "             'pink', 'navy', 'magenta', 'yellow', 'tomato',\n",
    "             'turquoise', 'yellowgreen', 'maroon', 'lightblue']\n",
    "mbow=[]\n",
    "mngram=[]\n",
    "mtfidf=[]\n",
    "mw2v=[]\n",
    "mglove=[]\n",
    "mft=[]\n",
    "\n",
    "def metricsAndROC(pred,metricsArray,rocTitle,colorIndex):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_sentiments, pred)\n",
    "    auc = metrics.roc_auc_score(test_sentiments, pred)\n",
    "    metricsArray.append(metrics.f1_score(test_sentiments, pred))\n",
    "    metricsArray.append(metrics.precision_score(test_sentiments, pred))\n",
    "    metricsArray.append(metrics.accuracy_score(test_sentiments, pred))\n",
    "    metricsArray.append(metrics.recall_score(test_sentiments, pred))\n",
    "    plt.plot(fpr, tpr,color=color[colorIndex], label=rocTitle)\n",
    "\n",
    "metricsAndROC(svm_bow_predictions,mbow,'SVM on BOW',0)\n",
    "metricsAndROC(svm_ngram_predictions,mngram,'SVM on NGRAM',1)\n",
    "metricsAndROC(svm_tfidf_predictions,mtfidf,'SVM on TFIDF',2)\n",
    "metricsAndROC(svm_wv_predictions,mw2v,'SVM on W2Vec(Gensim)',3)\n",
    "metricsAndROC(svm_glove_predictions,mglove,'SVM on Glove',4)\n",
    "metricsAndROC(svm_ft_predictions,mft,'SVM on FastText',5)\n",
    "\n",
    "#show the roc curve now\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Metrics comparison for SVM applied to various feature engineering methods- BOW, NGRAM, TFIDF, GLoVe, FastText"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_groups = 4\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.1\n",
    "opacity = 0.7\n",
    "error_config = {'ecolor': '0.3'}\n",
    "rects1 = ax.bar(index,mbow, bar_width, alpha=opacity, color=color[0], error_kw=error_config, label='BOW')\n",
    "\n",
    "z=index + bar_width\n",
    "rects2 = ax.bar(z, mngram, bar_width,alpha=opacity, color=color[1],error_kw=error_config,label='NGRAM')\n",
    "\n",
    "z=z+ bar_width\n",
    "rects3 = ax.bar(z, mtfidf, bar_width,alpha=opacity, color=color[2], error_kw=error_config, label='TFIDF')\n",
    "\n",
    "z=z+ bar_width\n",
    "rects4 = ax.bar(z,mw2v , bar_width,alpha=opacity, color=color[3], error_kw=error_config, label='W2V')\n",
    "\n",
    "z=z+ bar_width\n",
    "rects5 = ax.bar(z,mglove , bar_width,alpha=opacity, color=color[4], error_kw=error_config, label='Glove')\n",
    "\n",
    "z=z+ bar_width\n",
    "rects6 = ax.bar(z,mft , bar_width,alpha=opacity, color=color[5], error_kw=error_config, label='FastText')\n",
    "\n",
    "ax.set_xlabel('Metric')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Comparison of Feature Engineering Models on Amazon Reviews')\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "pltLabels=['F1','PRECISION','ACCURACY','RECALL']\n",
    "ax.set_xticklabels(pltLabels)\n",
    "ax.legend(bbox_to_anchor=(1, 0.5), loc=5, borderaxespad=0)\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}