{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Supervised Feature Engineeering of Reviews for Cellphone and Accessories category on Amazon "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import model_evaluation_utils as meu\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import warnings\n",
    "from sklearn.linear_model import  SGDClassifier\n",
    "from gensim.models.fasttext import FastText\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import utils\n",
    "import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "import model_evaluation_utils as meu\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nlp = spacy.load('en_vecs', parse=False, tag=False, entity=False)\n",
    "\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "\n",
    "\n",
    "NWORKERS=16\n",
    "\n",
    "NUMFEATURES=100\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load normalized data from processed file\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfdb = utils.readFromDisk('processed_partB')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample processed data loaded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Total Rows on processed dataset: ' + str(len(dfdb)))\n",
    "print('Sample of processed dataset. Notice the column named Clean_Review');\n",
    "dfdb.head(20)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split train and test data for both normalized and raw\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reviews = utils.readFromDisk('reviews')\n",
    "reviews_raw = utils.readFromDisk('reviews_raw')\n",
    "sentiments = utils.readFromDisk('sentiments')\n",
    "reviews_tokens = utils.readFromDisk('reviews_tokens')\n",
    "\n",
    "\n",
    "cutoff=round(len(dfdb)*0.75)\n",
    "# build train and test datasets\n",
    "train_reviews = reviews[:cutoff]\n",
    "train_reviews_raw = reviews_raw[:cutoff]\n",
    "train_reviews_tokens = reviews_tokens[:cutoff]\n",
    "\n",
    "train_sentiments = sentiments[:cutoff]\n",
    "train_sentiments=train_sentiments.astype('int')\n",
    "\n",
    "test_reviews = reviews[cutoff:]\n",
    "test_reviews_raw = reviews_raw[cutoff:]\n",
    "test_reviews_tokens = reviews_tokens[cutoff:]\n",
    "\n",
    "test_sentiments = sentiments[cutoff:]\n",
    "test_sentiments=test_sentiments.astype('int')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "utils.writeToDisk(train_reviews,'train_reviews')\n",
    "utils.writeToDisk(train_reviews_raw,'train_reviews_raw')\n",
    "utils.writeToDisk(train_reviews_tokens,'train_reviews_tokens')\n",
    "utils.writeToDisk(train_sentiments,'train_sentiments')\n",
    "\n",
    "utils.writeToDisk(test_reviews,'test_reviews')\n",
    "utils.writeToDisk(test_reviews_raw,'test_reviews_raw')\n",
    "utils.writeToDisk(test_reviews_tokens,'test_reviews_tokens')\n",
    "utils.writeToDisk(test_sentiments,'test_sentiments')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample train data loaded, notice Cleaned Review"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Total Rows on train dataset: ' + str(len(train_reviews)))\n",
    "print('Total Rows on test dataset: ' + str(len(test_reviews)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering using BOW"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# build BOW features on train reviews\n",
    "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\n",
    "cv_train_features = cv.fit_transform(train_reviews)\n",
    "# transform test reviews into features\n",
    "cv_test_features = cv.transform(test_reviews)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM Model Training, Prediction, Performance with BOW\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svm = SGDClassifier(loss='hinge', max_iter=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svm_bow_predictions = meu.train_predict_model(classifier=svm, train_features=cv_train_features, \n",
    "                                              train_labels=train_sentiments,test_features=cv_test_features, \n",
    "                                              test_labels=test_sentiments)\n",
    "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_bow_predictions,classes=[1, 0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering using NGRAM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# build ngram features on train reviews\n",
    "cvn = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(2,2))\n",
    "cvn_train_features = cvn.fit_transform(train_reviews)\n",
    "# transform test reviews into features\n",
    "cvn_test_features = cvn.transform(test_reviews)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM Model Training, Prediction, Performance with NGRAM\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svm_ngram_predictions = meu.train_predict_model(classifier=svm, train_features=cvn_train_features, train_labels=train_sentiments,test_features=cvn_test_features, test_labels=test_sentiments)\n",
    "print('NGRAM model:> Train features shape:', cvn_train_features.shape, ' Test features shape:', cvn_test_features.shape)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_ngram_predictions,classes=[1, 0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering using TFIDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# build TFIDF features on train reviews\n",
    "tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0, ngram_range=(1,2),\n",
    "                     sublinear_tf=True)\n",
    "tv_train_features = tv.fit_transform(train_reviews)\n",
    "tv_test_features = tv.transform(test_reviews)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM Model Training, Prediction,Performance with TFIDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svm_tfidf_predictions = meu.train_predict_model(classifier=svm, \n",
    "                                                train_features=tv_train_features, train_labels=train_sentiments,\n",
    "                                                test_features=tv_test_features, test_labels=test_sentiments)\n",
    "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_tfidf_predictions,classes=[1, 0])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prediction class label encoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "num_classes=2 \n",
    "# tokenize train reviews & encode train labels\n",
    "tn = ToktokTokenizer()\n",
    "tokenized_train = [tn.tokenize(text)                  for text in train_reviews]\n",
    "y_tr = le.fit_transform(train_sentiments)\n",
    "train_sentiments_encoded = keras.utils.to_categorical(y_tr, num_classes)\n",
    "utils.writeToDisk(tokenized_train,'tokenized_train')\n",
    "utils.writeToDisk(train_sentiments_encoded,'train_sentiments_encoded')\n",
    "\n",
    "\n",
    "# tokenize test reviews & encode test labels\n",
    "tokenized_test = [tn.tokenize(text)                   for text in test_reviews]\n",
    "y_ts = le.fit_transform(test_sentiments)\n",
    "test_sentiments_encoded = keras.utils.to_categorical(y_ts, num_classes)\n",
    "utils.writeToDisk(tokenized_train,'tokenized_test')\n",
    "utils.writeToDisk(test_sentiments_encoded,'test_sentiments_encoded')\n",
    "\n",
    "utils.writeToDisk(le,'label_encoder')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering with word embeddings (Word2Vec/Gensim)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# build word2vec model using gensim\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(tokenized_train,workers=NWORKERS)    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def averaged_word2vec_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    \n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "        nwords = 0.\n",
    "        \n",
    "        for word in words:\n",
    "            if word in vocabulary: \n",
    "                nwords = nwords + 1.\n",
    "                feature_vector = np.add(feature_vector, model[word])\n",
    "        if nwords:\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "        return feature_vector\n",
    "\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# generate averaged word vector features from word2vec model\n",
    "avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model,num_features=NUMFEATURES)\n",
    "avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model,num_features=NUMFEATURES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM Model Training, Prediction, Performance with Word2Vec\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svm_wv_predictions = meu.train_predict_model(classifier=svm, \n",
    "                                                train_features=avg_wv_train_features, train_labels=train_sentiments,\n",
    "                                                test_features=avg_wv_test_features, test_labels=test_sentiments)\n",
    "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape, ' Test features shape:', avg_wv_test_features.shape)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_wv_predictions,classes=[1, 0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM Model Training, Prediction, Performance with GLoVe\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# feature engineering with GloVe model\n",
    "train_nlp = [nlp(item) for item in train_reviews_raw]\n",
    "train_glove_features = np.array([item.vector for item in train_nlp])\n",
    "utils.writeToDisk(train_glove_features,'train_glove_features')\n",
    "\n",
    "test_nlp = [nlp(item) for item in test_reviews_raw]\n",
    "test_glove_features = np.array([item.vector for item in test_nlp])\n",
    "utils.writeToDisk(test_glove_features,'test_glove_features')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svm_glove_predictions = meu.train_predict_model(classifier=svm, \n",
    "                                                train_features=train_glove_features, train_labels=train_sentiments,\n",
    "                                                test_features=test_glove_features, test_labels=test_sentiments)\n",
    "print('Glove model:> Train features shape:', train_glove_features.shape, ' Test features shape:', test_glove_features.shape)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_glove_predictions,classes=[1, 0])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM Model Training, Prediction, Performance with FastText (Reduced dataset due to memory constraints)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sg decides whether to use the skip-gram model (1) or CBOW (0) \n",
    "#ft_model = FastText( tokenized_train1, size = ft_num_features, window = 50,min_count = 5,\n",
    "#                     sample = 1e-3, sg = 1, iter = max_iter, workers = NWORKERS) \n",
    "ft_model = FastText( tokenized_train, size = NUMFEATURES,workers = NWORKERS) \n",
    "# generate averaged word vector features from word2vec model \n",
    "train_ft_features = averaged_word2vec_vectorizer( corpus = tokenized_train, num_features=NUMFEATURES,model = ft_model) \n",
    "test_ft_features = averaged_word2vec_vectorizer( corpus = tokenized_test,num_features=NUMFEATURES, model = ft_model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svm_ft_predictions = meu.train_predict_model(classifier=svm, \n",
    "                                                train_features=train_ft_features, train_labels=train_sentiments,\n",
    "                                                test_features=test_ft_features, test_labels=test_sentiments)\n",
    "print('FastText:> Train features shape:', train_ft_features.shape, ' Test features shape:', test_ft_features.shape)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_ft_predictions,classes=[1, 0])\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ROC curves for SVM applied to various feature engineering methods- BOW, NGRAM, TFIDF, GLoVe, FastText"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(0).clf()\n",
    "\n",
    "color = ['blue', 'orange', 'red', 'green', 'coral',\n",
    "             'grey', 'indigo', 'gold', 'lime', 'olive',\n",
    "             'pink', 'navy', 'magenta', 'yellow', 'tomato',\n",
    "             'turquoise', 'yellowgreen', 'maroon', 'lightblue']\n",
    "mbow=[]\n",
    "mngram=[]\n",
    "mtfidf=[]\n",
    "mw2v=[]\n",
    "mglove=[]\n",
    "mft=[]\n",
    "\n",
    "def metricsAndROC(pred,metricsArray,rocTitle,colorIndex):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_sentiments, pred)\n",
    "    auc = metrics.roc_auc_score(test_sentiments, pred)\n",
    "    metricsArray.append(metrics.f1_score(test_sentiments, pred))\n",
    "    metricsArray.append(metrics.precision_score(test_sentiments, pred))\n",
    "    metricsArray.append(metrics.accuracy_score(test_sentiments, pred))\n",
    "    metricsArray.append(metrics.recall_score(test_sentiments, pred))\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr,color=color[colorIndex], label=rocTitle.format(auc))\n",
    "\n",
    "\n",
    "def metricsAndROC0(pred,metricsArray,rocTitle,colorIndex):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_sentiments, pred)\n",
    "    auc = metrics.roc_auc_score(test_sentiments, pred)\n",
    "    metricsArray.append(metrics.f1_score(test_sentiments, pred))\n",
    "    metricsArray.append(metrics.precision_score(test_sentiments, pred))\n",
    "    metricsArray.append(metrics.accuracy_score(test_sentiments, pred))\n",
    "    metricsArray.append(metrics.recall_score(test_sentiments, pred))\n",
    "    plt.plot(fpr, tpr,color=color[colorIndex], label=rocTitle)\n",
    "\n",
    "metricsAndROC(svm_bow_predictions,mbow,'SVM on BOW',0)\n",
    "metricsAndROC(svm_ngram_predictions,mngram,'SVM on NGRAM',1)\n",
    "metricsAndROC(svm_tfidf_predictions,mtfidf,'SVM on TFIDF',2)\n",
    "metricsAndROC(svm_wv_predictions,mw2v,'SVM on W2Vec(Gensim)',3)\n",
    "metricsAndROC(svm_glove_predictions,mglove,'SVM on Glove',4)\n",
    "metricsAndROC(svm_ft_predictions,mft,'SVM on FastText',5)\n",
    "\n",
    "#show the roc curve now\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "# show the legend\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Metrics comparison for SVM applied to various feature engineering methods- BOW, NGRAM, TFIDF, GLoVe, FastText"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_groups = 4\n",
    "index = np.arange(n_groups)\n",
    "bar_width = .1\n",
    "\n",
    "plt.bar(index,mbow, bar_width, color=color[0], label='BOW')\n",
    "\n",
    "z=index + bar_width\n",
    "plt.bar(z, mngram, bar_width, color=color[1],label='NGRAM')\n",
    "\n",
    "\n",
    "\n",
    "z=z+ bar_width\n",
    "plt.bar(z, mtfidf, bar_width, color=color[2], label='TFIDF')\n",
    "\n",
    "z=z+ bar_width\n",
    "plt.bar(z,mw2v , bar_width, color=color[3], label='W2V')\n",
    "\n",
    "z=z+ bar_width\n",
    "plt.bar(z,mglove , bar_width,color=color[4], label='Glove')\n",
    "\n",
    "z=z+ bar_width\n",
    "plt.bar(z,mft , bar_width, color=color[5], label='FastText')\n",
    "\n",
    "\n",
    "#ax.set_xlabel('Metric')\n",
    "#ax.set_ylabel('Value')\n",
    "#ax.set_title('Comparison of Feature Engineering Models on Amazon Reviews')\n",
    "#ax.set_xticks(index + bar_width / 2)\n",
    "pltLabels=['F1','PRECISION','ACCURACY','RECALL']\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('group', fontweight='bold')\n",
    "plt.xticks([r + bar_width for r in range(n_groups)], pltLabels)\n",
    " \n",
    "# Create legend & Show graphic\n",
    "plt.legend(frameon=False,ncol=3, loc='lower left')\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Similar words and Visualize word embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# view similar words based on gensim's model\n",
    "similar_words = {search_term: [item[0] for item in w2v_model.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['battery', 'screen', 'time', 'camera', 'mobile', 'app', 'price']}\n",
    "similar_words\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# visualize embeddings by Word2Vec using Gensim\n",
    "\n",
    "words = sum([[k] + v for k, v in similar_words.items()], [])\n",
    "wvs = w2v_model.wv[words]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}