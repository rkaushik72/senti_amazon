{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import necessary depencencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dropout, Activation, Dense\n",
    "from keras.models import Sequential\n",
    "import model_evaluation_utils as meu\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, SpatialDropout1D\n",
    "from keras.layers import LSTM\n",
    "import xgboost as xgb\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "np.set_printoptions(precision=2, linewidth=80)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load normalized data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train and test datasets\n",
    "train_reviews = utils.readFromDisk('train_reviews')\n",
    "train_sentiments = utils.readFromDisk('train_sentiments')\n",
    "test_reviews = utils.readFromDisk('test_reviews')\n",
    "test_sentiments = utils.readFromDisk('test_sentiments')\n",
    "tokenized_train = utils.readFromDisk('tokenized_train')\n",
    "tokenized_test = utils.readFromDisk('tokenized_test')\n",
    "train_sentiments_encoded = utils.readFromDisk('train_sentiments_encoded')\n",
    "test_sentiments_encoded = utils.readFromDisk('test_sentiments_encoded')\n",
    "\n",
    "#glove features\n",
    "# feature engineering with GloVe model\n",
    "train_glove_features = utils.readFromDisk('train_glove_features')\n",
    "test_glove_features = utils.readFromDisk('test_glove_features')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', max_iter=500, C=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression - Training and Prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Logistic Regression model\n",
    "lr_predictions = meu.train_predict_model(classifier=lr, \n",
    "                                             train_features=train_glove_features, train_labels=train_sentiments,\n",
    "                                             test_features=test_glove_features, test_labels=test_sentiments)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression - Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_predictions,\n",
    "                                      classes=[1,0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svm = SGDClassifier(loss='hinge', max_iter=500)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVM - Training and Prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svm_predictions = meu.train_predict_model(classifier=svm, \n",
    "                                             train_features=train_glove_features, train_labels=train_sentiments,\n",
    "                                             test_features=test_glove_features, test_labels=test_sentiments)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVM - Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_predictions,\n",
    "                                      classes=[1,0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Deep neural network architecture (DNN)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def construct_deepnn_architecture(num_input_features):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(512, activation='relu', input_shape=(num_input_features,)))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(512, activation='relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(512, activation='relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(2))\n",
    "    dnn_model.add(Activation('softmax'))\n",
    "\n",
    "    dnn_model.compile(loss='categorical_crossentropy', optimizer='adam',                 \n",
    "                      metrics=['accuracy'])\n",
    "    return dnn_model\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DNN - Training and Prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dnn = construct_deepnn_architecture(num_input_features=300)\n",
    "\n",
    "try:   \n",
    "    dnn=utils.readFromDisk('dnn')\n",
    "except FileNotFoundError as te:    \n",
    "    batch_size = 100\n",
    "    dnn.fit(train_glove_features, train_sentiments_encoded, epochs=5, batch_size=batch_size, \n",
    "            shuffle=True, validation_split=0.1, verbose=1)\n",
    "    utils.writeToDisk(dnn,'dnn')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "le = utils.readFromDisk('label_encoder')\n",
    "y_pred = dnn.predict_classes(test_glove_features)\n",
    "dnn_predictions = le.inverse_transform(y_pred) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DNN - Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=dnn_predictions, \n",
    "                                      classes=[1,0])  \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# XGBoost"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xg = xgb.XGBClassifier(n_estimators=500, max_depth=5, base_score=0.5,objective='binary:logistic', random_state=42,nthread=16)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost - Training and Prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:   \n",
    "    xg=utils.readFromDisk('xg')\n",
    "except FileNotFoundError as te:    \n",
    "    xg.fit(train_glove_features, train_sentiments)\n",
    "    utils.writeToDisk(xg,'xg')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xg_predictions= xg.predict(test_glove_features)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost - Performance\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=xg_predictions, \n",
    "                                      classes=[1,0])  \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LSTM\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# build word to index vocabulary\n",
    "token_counter = Counter([token for review in tokenized_train for token in review])\n",
    "vocab_map = {item[0]: index+1 for index, item in enumerate(dict(token_counter).items())}\n",
    "max_index = np.max(list(vocab_map.values()))\n",
    "vocab_map['PAD_INDEX'] = 0\n",
    "vocab_map['NOT_FOUND_INDEX'] = max_index+1\n",
    "vocab_size = len(vocab_map)\n",
    "# view vocabulary size and part of the vocabulary map\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Sample slice of vocabulary map:', dict(list(vocab_map.items())[10:20]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM- Encode and Pad datasets & Encode prediction class labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get max length of train corpus and initialize label encoder\n",
    "max_len = np.max([len(review) for review in tokenized_train])\n",
    "\n",
    "## Train reviews data corpus\n",
    "# Convert tokenized text reviews to numeric vectors\n",
    "train_X = [[vocab_map[token] for token in tokenized_review] for tokenized_review in tokenized_train]\n",
    "train_X = sequence.pad_sequences(train_X, maxlen=max_len) # pad \n",
    "\n",
    "## Test reviews data corpus\n",
    "# Convert tokenized text reviews to numeric vectors\n",
    "test_X = [[vocab_map[token] if vocab_map.get(token) else vocab_map['NOT_FOUND_INDEX'] \n",
    "           for token in tokenized_review] \n",
    "              for tokenized_review in tokenized_test]\n",
    "test_X = sequence.pad_sequences(test_X, maxlen=max_len)\n",
    "\n",
    "# view vector shapes\n",
    "print('Max length of train review vectors:', max_len)\n",
    "print('Train review vectors shape:', train_X.shape, ' Test review vectors shape:', test_X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM- Build, train and visualize the LSTM Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128 # dimension for dense embeddings for each token\n",
    "LSTM_DIM = 64 # total LSTM units\n",
    "\n",
    "lstm = Sequential()\n",
    "\n",
    "try:   \n",
    "    lstm=utils.readFromDisk('lstm')\n",
    "    print(lstm.summary())\n",
    "except FileNotFoundError as te:    \n",
    "    lstm.add(Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_len))\n",
    "    lstm.add(SpatialDropout1D(0.2))\n",
    "    lstm.add(LSTM(LSTM_DIM, dropout=0.2, recurrent_dropout=0.2))\n",
    "    lstm.add(Dense(2, activation=\"sigmoid\"))\n",
    "    \n",
    "    lstm.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    print(lstm.summary())\n",
    "\n",
    "    SVG(model_to_dot(lstm, show_shapes=True, show_layer_names=False, \n",
    "                     rankdir='LR').create(prog='dot', format='svg'))\n",
    "    batch_size = 100\n",
    "    lstm.fit(train_X, train_sentiments_encoded, epochs=2, batch_size=batch_size, \n",
    "              shuffle=True, validation_split=0.1, verbose=1)\n",
    "\n",
    "    utils.writeToDisk(lstm,'lstm')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM - Predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lstm_pred_test = lstm.predict_classes(test_X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lstm_predictions = le.inverse_transform(lstm_pred_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM - Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lstm_predictions, \n",
    "                                      classes=[1,0])  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ensemble Stacking models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n",
    "                          meta_classifier=lr)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensemble Stacking - n fold validations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf_list = [clf1, clf2, clf3, sclf]\n",
    "scores_list=[]\n",
    "\n",
    "try:\n",
    "    clf_list=utils.readFromDisk('clf_list');\n",
    "    scores_list=utils.readFromDisk('scores_list');\n",
    "except FileNotFoundError as te:    \n",
    "    for clf in clf_list:\n",
    "        scores = cross_val_score(clf, train_glove_features, train_sentiments, cv=3, scoring='accuracy',n_jobs=-1)\n",
    "        scores_list.append(scores)\n",
    "    utils.writeToDisk(clf_list,'clf_list')\n",
    "    utils.writeToDisk(scores_list,'scores_list')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensemble Stacking - metrics and visualizations\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "label = ['KNN', 'Random Forest', 'Naive Bayes', 'Stacking Classifier']\n",
    "\n",
    "clf_cv_mean = []\n",
    "clf_cv_std = []\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "for clf, label, grd, scores in zip(clf_list, label, grid,scores_list):\n",
    "\n",
    "\n",
    "    print (\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "    clf_cv_mean.append(scores.mean())\n",
    "    clf_cv_std.append(scores.std())\n",
    "        \n",
    "    clf.fit(train_glove_features, train_sentiments)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=train_glove_features, y=train_sentiments, clf=clf)\n",
    "    plt.title(label)\n",
    "\n",
    "plt.show()     \n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# All Models Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model ROC curves"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(0).clf()\n",
    "\n",
    "color = ['blue', 'orange', 'red', 'green', 'coral',\n",
    "             'grey', 'indigo', 'gold', 'lime', 'olive',\n",
    "             'pink', 'navy', 'magenta', 'yellow', 'tomato',\n",
    "             'turquoise', 'yellowgreen', 'maroon', 'lightblue']\n",
    "mlr=[]\n",
    "msvm=[]\n",
    "mdnn=[]\n",
    "mxg=[]\n",
    "mlstm=[]\n",
    "\n",
    "def metricsAndROC(pred,metricsArray,rocTitle,colorIndex):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_sentiments, pred)\n",
    "    auc = metrics.roc_auc_score(test_sentiments, pred)\n",
    "    metricsArray.append(metrics.f1_score(test_sentiments, pred))\n",
    "    metricsArray.append(metrics.precision_score(test_sentiments, pred))\n",
    "    metricsArray.append(metrics.accuracy_score(test_sentiments, pred))\n",
    "    metricsArray.append(metrics.recall_score(test_sentiments, pred))\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr,color=color[colorIndex], label=rocTitle.format(auc))\n",
    "\n",
    "\n",
    "\n",
    "metricsAndROC(lr_predictions,mlr,'LR',0)\n",
    "metricsAndROC(svm_predictions,msvm,'SVM',1)\n",
    "metricsAndROC(svm_predictions,mdnn,'DNN',2)\n",
    "metricsAndROC(xg_predictions,mxg,'XGBoost',3)\n",
    "metricsAndROC(lstm_predictions,mlstm,'LSTM',4)\n",
    "\n",
    "\n",
    "\n",
    "#show the roc curve now\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "# show the legend\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## All Models Metrics comparison"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_groups = 4\n",
    "index = np.arange(n_groups)\n",
    "bar_width = .1\n",
    "\n",
    "plt.bar(index,mlr, bar_width, color=color[0], label='LR')\n",
    "\n",
    "\n",
    "z=index + bar_width\n",
    "plt.bar(z, msvm, bar_width, color=color[1],label='SVM')\n",
    "\n",
    "\n",
    "z=z+ bar_width\n",
    "plt.bar(z, mdnn, bar_width, color=color[2], label='DNN')\n",
    "\n",
    "z=z+ bar_width\n",
    "plt.bar(z,mxg , bar_width, color=color[3], label='XGB')\n",
    "\n",
    "z=z+ bar_width\n",
    "plt.bar(z,mlstm , bar_width,color=color[4], label='LSTM')\n",
    "\n",
    "\n",
    "\n",
    "#ax.set_xlabel('Metric')\n",
    "#ax.set_ylabel('Value')\n",
    "#ax.set_title('Comparison of Feature Engineering Models on Amazon Reviews')\n",
    "#ax.set_xticks(index + bar_width / 2)\n",
    "pltLabels=['F1','PRECISION','ACCURACY','RECALL']\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('Model Metrics', fontweight='bold')\n",
    "plt.xticks([r + bar_width for r in range(n_groups)], pltLabels)\n",
    " \n",
    "# Create legend & Show graphic\n",
    "plt.legend(frameon=False,ncol=3, loc='lower left')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}