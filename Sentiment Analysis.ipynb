{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import necessary depencencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dropout, Activation, Dense\n",
    "from keras.models import Sequential\n",
    "import model_evaluation_utils as meu\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, SpatialDropout1D\n",
    "from keras.layers import LSTM\n",
    "import xgboost as xgb\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, cross_val_predict, validation_curve\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from collections import Counter\n",
    "from distutils.version import LooseVersion as Version\n",
    "from sklearn import __version__ as sklearn_version\n",
    "#from scipy.misc import comb\n",
    "from scipy.special import comb\n",
    "import math\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import utils\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals import six\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import _name_estimators\n",
    "import numpy as np\n",
    "import operator\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "if Version(sklearn_version) < '0.18':\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "else:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.pipeline import Pipeline\n",
    "if Version(sklearn_version) < '0.18':\n",
    "    from sklearn.cross_validation import cross_val_score\n",
    "else:\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "NWORKERS=16"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load normalized data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train and test datasets\n",
    "train_reviews = utils.readFromDisk('train_reviews')\n",
    "train_sentiments = utils.readFromDisk('train_sentiments')\n",
    "test_reviews = utils.readFromDisk('test_reviews')\n",
    "test_sentiments = utils.readFromDisk('test_sentiments')\n",
    "tokenized_train = utils.readFromDisk('tokenized_train')\n",
    "tokenized_test = utils.readFromDisk('tokenized_test')\n",
    "train_sentiments_encoded = utils.readFromDisk('train_sentiments_encoded')\n",
    "test_sentiments_encoded = utils.readFromDisk('test_sentiments_encoded')\n",
    "\n",
    "\n",
    "train_features = utils.readFromDisk('train_ft_features')\n",
    "test_features = utils.readFromDisk('test_ft_features')\n",
    "\n",
    "#labelencoder used for encoding in feature engg\n",
    "le = utils.readFromDisk('label_encoder')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#split train into train and vaildate sets for models that need it\n",
    "def splitTrainForValidation(train_x,train_y,val_ratio):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=val_ratio)\n",
    "    return X_train, X_val, y_train, y_val\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression (hyperparameter tuning and caching)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#validation curves\n",
    "param_name = 'C'\n",
    "param_range=np.linspace(1, 15, 4,dtype=int)\n",
    "classifier=LogisticRegression()\n",
    "title=param_name\n",
    "\n",
    "meu.validationCurve(classifier,train_features,train_sentiments,param_name,param_range,NWORKERS,title)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    lr=utils.readFromDisk('lr')\n",
    "except FileNotFoundError as te:\n",
    "    lr0 = LogisticRegression()\n",
    "    lr_hyperparameters = {    \n",
    "        'C' : [5],\n",
    "        #'penalty' : ['l2'],\n",
    "        'max_iter':[1000]\n",
    "    }\n",
    "    lr_clf = RandomizedSearchCV(lr0, lr_hyperparameters,cv=5, n_iter=10,n_jobs=-1)\n",
    "\n",
    "    # Fit grid search\n",
    "    lr = lr_clf.fit(train_features,train_sentiments)\n",
    "\n",
    "    utils.writeToDisk(lr,'lr')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    # View best hyperparameters\n",
    "    lr.best_estimator_.get_params()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Predict target vector\n",
    "lr_predictions=lr.predict(test_features)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression - Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_predictions,\n",
    "                                      classes=[1,0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression - Learning Curve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.learningCurve(train_features,train_sentiments,lr.best_estimator_,5,np.linspace(.1, 0.7, 7))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM using sgd classifier (hyperparameter tuning and caching)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#validation curves\n",
    "param_name = 'alpha'\n",
    "param_range=[0.00001,0.0001, 0.001, 0.01, 0.1]\n",
    "classifier=SGDClassifier()\n",
    "title=param_name\n",
    "\n",
    "meu.validationCurve(classifier,train_features,train_sentiments,param_name,param_range,NWORKERS,title)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    svm=utils.readFromDisk('svm')\n",
    "except FileNotFoundError as te:\n",
    "    #svm0 = SGDClassifier(max_iter=500,n_jobs=NWORKERS)\n",
    "    svm0 = SGDClassifier()\n",
    "\n",
    "    svm_n_jobs=[NWORKERS]\n",
    "    svm_max_iter=[1000]\n",
    "    svm_loss= [\"hinge\"]\n",
    "    svm_alpha = [0.00001]\n",
    "    svm_penalty = [\"l2\"]\n",
    "    svm_hyperparameters = dict(max_iter=[1000],loss=svm_loss, alpha=svm_alpha,penalty=svm_penalty,n_jobs=svm_n_jobs)\n",
    "\n",
    "    svm_clf = GridSearchCV(svm0, svm_hyperparameters,cv=5, n_jobs=1)\n",
    "\n",
    "    svm = svm_clf.fit(train_features,train_sentiments)\n",
    "\n",
    "    utils.writeToDisk(svm,'svm')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    svm.best_estimator_.get_params()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svm_predictions=svm.predict(test_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVM - Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_predictions,\n",
    "                                      classes=[1,0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVM - Learning Curve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.learningCurve(train_features,train_sentiments,svm.best_estimator_,5,np.linspace(.1, 0.7, 7))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Naive Bayes (hyperparameter tuning and caching)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    nb=utils.readFromDisk('nb')\n",
    "except FileNotFoundError as te:\n",
    "    nb0 = GaussianNB()\n",
    "\n",
    "    nb_hyperparameters = {\n",
    "        #'priors': np.linspace(0.1, 0.9, 6)\n",
    "       #'fit_prior': [True, False],  \n",
    "    }\n",
    "    nb_clf = GridSearchCV(nb0,nb_hyperparameters, cv=5, n_jobs=-1)\n",
    "\n",
    "    nb = nb_clf.fit(train_features,train_sentiments)\n",
    "\n",
    "    utils.writeToDisk(nb,'nb')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    nb.best_estimator_.get_params()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nb_predictions=nb.predict(test_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes  - Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=nb_predictions,\n",
    "                                      classes=[1,0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes - Learning Curve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.learningCurve(train_features,train_sentiments,nb.best_estimator_,5,np.linspace(.1, 0.7, 7))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest (hyperparameter tuning and caching)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#validation curves\n",
    "param_name = 'n_estimators'\n",
    "param_range=[10, 50, 100, 150, 200]\n",
    "classifier=RandomForestClassifier()\n",
    "title=param_name\n",
    "\n",
    "meu.validationCurve(classifier,train_features,train_sentiments,param_name,param_range,NWORKERS,title)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#validation curves\n",
    "param_name = 'min_samples_leaf'\n",
    "param_range=[1,3,5]\n",
    "classifier=RandomForestClassifier()\n",
    "title=param_name\n",
    "\n",
    "meu.validationCurve(classifier,train_features,train_sentiments,param_name,param_range,NWORKERS,title)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#validation curves\n",
    "\n",
    "param_name = 'max_depth'\n",
    "param_range=[200, 500, 1000]\n",
    "classifier=RandomForestClassifier()\n",
    "title=param_name\n",
    "\n",
    "meu.validationCurve(classifier,train_features,train_sentiments,param_name,param_range,NWORKERS,title)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#validation curves\n",
    "param_name = 'min_samples_split'\n",
    "param_range=[2,3,4]\n",
    "classifier=RandomForestClassifier()\n",
    "title=param_name\n",
    "\n",
    "meu.validationCurve(classifier,train_features,train_sentiments,param_name,param_range,NWORKERS,title)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#hyper parameter tuning\n",
    "try:\n",
    "    rf=utils.readFromDisk('rf')\n",
    "except FileNotFoundError as te:\n",
    "    rf0 = RandomForestClassifier()\n",
    "\n",
    "    rf_hyperparameters = {\n",
    "        'n_estimators': [50],\n",
    "        'max_features': ['auto'],\n",
    "        'min_samples_leaf': [1],\n",
    "        'random_state':[1],\n",
    "        'oob_score': [True],\n",
    "        'max_depth':[200],\n",
    "        'min_samples_split':[2],\n",
    "        'n_jobs':[NWORKERS]\n",
    "    }\n",
    "    rf_clf = RandomizedSearchCV(rf0,rf_hyperparameters, cv=5, n_jobs=NWORKERS)\n",
    "\n",
    "    rf = rf_clf.fit(train_features,train_sentiments)\n",
    "\n",
    "    utils.writeToDisk(rf,'rf')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    rf.best_estimator_.get_params()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_predictions=rf.predict(test_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest  - Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=rf_predictions,\n",
    "                                      classes=[1,0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest - Learning Curve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.learningCurve(train_features,train_sentiments,rf.best_estimator_,5,np.linspace(.1, 0.7, 7))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# KNN (hyperparameter tuned)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#validation curves\n",
    "param_name = 'n_neighbors'\n",
    "param_range=[1,2,3,4,5]\n",
    "classifier=KNeighborsClassifier()\n",
    "title=param_name\n",
    "\n",
    "meu.validationCurve(classifier,train_features,train_sentiments,param_name,param_range,NWORKERS,title)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    knn=utils.readFromDisk('knn')\n",
    "except FileNotFoundError as te:\n",
    "    knn0 = KNeighborsClassifier()\n",
    "\n",
    "    knn_hyperparameters = {\n",
    "        'n_neighbors': [1]\n",
    "    }\n",
    "    knn_clf = GridSearchCV(knn0,knn_hyperparameters, cv=5, n_jobs=-1)\n",
    "\n",
    "    knn = knn_clf.fit(train_features,train_sentiments)\n",
    "\n",
    "    utils.writeToDisk(knn,'knn')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    knn.best_estimator_.get_params()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "knn_predictions=knn.predict(test_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## KNN  - Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=knn_predictions,\n",
    "                                      classes=[1,0])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## KNN - Learning Curve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.learningCurve(train_features,train_sentiments,knn.best_estimator_,5,np.linspace(.1, 0.7, 7))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# XGBoost (hyperparameter tuning and caching)\n",
    "\n",
    "\n",
    "Parameters\n",
    "max_depth (int) – Maximum tree depth for base learners.\n",
    "\n",
    "learning_rate (float) – Boosting learning rate (xgb’s “eta”)\n",
    "\n",
    "n_estimators (int) – Number of trees to fit.\n",
    "\n",
    "verbosity (int) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
    "\n",
    "objective (string or callable) – Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).\n",
    "\n",
    "booster (string) – Specify which booster to use: gbtree, gblinear or dart.\n",
    "\n",
    "tree_method (string) – Specify which tree method to use. Default to auto. If this parameter is set to default, XGBoost will choose the most conservative option available. It’s recommended to study this option from parameters document.\n",
    "\n",
    "n_jobs (int) – Number of parallel threads used to run xgboost.\n",
    "\n",
    "gamma (float) – Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "\n",
    "min_child_weight (int) – Minimum sum of instance weight(hessian) needed in a child.\n",
    "\n",
    "max_delta_step (int) – Maximum delta step we allow each tree’s weight estimation to be.\n",
    "\n",
    "subsample (float) – Subsample ratio of the training instance.\n",
    "\n",
    "colsample_bytree (float) – Subsample ratio of columns when constructing each tree.\n",
    "\n",
    "colsample_bylevel (float) – Subsample ratio of columns for each level.\n",
    "\n",
    "colsample_bynode (float) – Subsample ratio of columns for each split.\n",
    "\n",
    "reg_alpha (float (xgb's alpha)) – L1 regularization term on weights\n",
    "\n",
    "reg_lambda (float (xgb's lambda)) – L2 regularization term on weights\n",
    "\n",
    "scale_pos_weight (float) – Balancing of positive and negative weights.\n",
    "\n",
    "base_score – The initial prediction score of all instances, global bias.\n",
    "\n",
    "random_state (int) –\n",
    "\n",
    "Random number seed.\n",
    "\n",
    "Note\n",
    "\n",
    "Using gblinear booster with shotgun updater is nondeterministic as it uses Hogwild algorithm.\n",
    "\n",
    "missing (float, optional) – Value in the data which needs to be present as a missing value. If None, defaults to np.nan.\n",
    "\n",
    "num_parallel_tree (int) – Used for boosting random forest.\n",
    "\n",
    "importance_type (string, default \"gain\") – The feature importance type for the feature_importances_ property: either “gain”, “weight”, “cover”, “total_gain” or “total_cover”."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#validation curves\n",
    "param_name = 'max_depth'\n",
    "param_range=np.linspace(10, 200, 5)\n",
    "classifier=xgb.XGBClassifier()\n",
    "title=param_name\n",
    "\n",
    "meu.validationCurve(classifier,train_features,train_sentiments,param_name,param_range,NWORKERS,title)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#validation curves\n",
    "param_name = 'n_estimators'\n",
    "param_range=np.linspace(10, 50, 100)\n",
    "classifier=xgb.XGBClassifier()\n",
    "title=param_name\n",
    "\n",
    "meu.validationCurve(classifier,train_features,train_sentiments,param_name,param_range,NWORKERS,title)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#validation curves\n",
    "param_name = 'learning_rate'\n",
    "param_range=[0.01, 0.05,0.1]\n",
    "classifier=xgb.XGBClassifier()\n",
    "title=param_name\n",
    "\n",
    "meu.validationCurve(classifier,train_features,train_sentiments,param_name,param_range,NWORKERS,title)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    xg=utils.readFromDisk('xg')\n",
    "except FileNotFoundError as te:\n",
    "#    xg = xgb.XGBClassifier(n_estimators=500, max_depth=5, base_score=0.5,objective='binary:logistic', \n",
    "#                       random_state=42,nthread=16,n_jobs=NWORKERS)\n",
    "    xg0 = xgb.XGBClassifier()\n",
    "\n",
    "    xg_hyperparameters = {\n",
    "        'max_depth': [200],\n",
    "        'n_estimators': [50],\n",
    "        'learning_rate': [0.01],\n",
    "        'objective': ['binary:logistic'],\n",
    "        'random_state': [42],\n",
    "        'n_jobs': [NWORKERS]\n",
    "    }\n",
    "\n",
    "    xg_clf = GridSearchCV(xg0, xg_hyperparameters,cv=5, n_jobs=1)\n",
    "    \n",
    "    xg=xg_clf.fit(train_features, train_sentiments)\n",
    "\n",
    "    utils.writeToDisk(xg,'xg')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    xg.best_estimator_.get_params()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xg_predictions= xg.predict(test_features)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost - Performance\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=xg_predictions, \n",
    "                                      classes=[1,0])  \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost - Learning Curve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.learningCurve(train_features,train_sentiments,xg.best_estimator_,5,np.linspace(.1, 0.7, 7))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Deep neural network architecture (DNN) (hyperparameter tuning and caching)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#validation curves\n",
    "param_name = ''\n",
    "param_range=[]\n",
    "classifier=\n",
    "title=param_name\n",
    "\n",
    "meu.validationCurve(classifier,train_features,train_sentiments,param_name,param_range,NWORKERS,title)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_dnn_model(optimizer='adam', init='glorot_uniform'):    \n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(512, activation='relu', input_shape=(300,)))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(512, activation='relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(512, activation='relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(2))\n",
    "    dnn_model.add(Activation('softmax'))\n",
    "\n",
    "    dnn_model.compile(loss='categorical_crossentropy', optimizer=optimizer,                 \n",
    "                      metrics=['accuracy'])\n",
    "    return dnn_model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DNN - Training and Prediction\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "try:   \n",
    "    dnn=utils.readFromDisk('dnn')\n",
    "except FileNotFoundError as te:\n",
    "    #see https://machinelearningmastery.com/use-keras-deep-learning-models-scikit-learn-python/\n",
    "    dnn0 = KerasClassifier(build_fn = create_dnn_model)\n",
    "\n",
    "    # grid search epochs, batch size and optimizer\n",
    "    #dnn_optimizers = ['rmsprop', 'adam']\n",
    "    #dnn_init = ['glorot_uniform', 'normal', 'uniform']\n",
    "    dnn_epochs = [10,25,50]\n",
    "    dnn_batch_size=[50,100,200,300]\n",
    "    dnn_validation_split=[0.1]\n",
    "    dnn_shuffle=[True]\n",
    "    dnn_workers=[NWORKERS]\n",
    "    dnn_use_multiprocessing=[True]\n",
    "    dnn_verbose=[1]\n",
    "    dnn_hyperparameters = dict(epochs=dnn_epochs, \n",
    "                               batch_size=dnn_batch_size,validation_split=dnn_validation_split,\n",
    "                               shuffle=dnn_shuffle, workers=dnn_workers, use_multiprocessing=dnn_use_multiprocessing,\n",
    "                               verbose=dnn_verbose)\n",
    "    \n",
    "    \n",
    "    dnn_clf = GridSearchCV(dnn0, dnn_hyperparameters,cv=5, n_jobs=1)\n",
    "   \n",
    "    dnn=dnn_clf.fit(train_features,train_sentiments_encoded)    \n",
    "    \n",
    "    utils.writeToDisk(dnn,'dnn')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    dnn.best_estimator_.get_params()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = dnn.predict(test_features)\n",
    "dnn_predictions = le.inverse_transform(y_pred) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DNN - Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=dnn_predictions, \n",
    "                                      classes=[1,0])  \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DNN - Learning Curve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.learningCurve(train_features,train_sentiments,dnn.best_estimator_,5,np.linspace(.1, 0.7, 7))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LSTM\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#validation curves\n",
    "param_name = ''\n",
    "param_range=[]\n",
    "classifier=\n",
    "title=param_name\n",
    "\n",
    "meu.validationCurve(classifier,train_features,train_sentiments,param_name,param_range,NWORKERS,title)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# build word to index vocabulary\n",
    "token_counter = Counter([token for review in tokenized_train for token in review])\n",
    "vocab_map = {item[0]: index+1 for index, item in enumerate(dict(token_counter).items())}\n",
    "max_index = np.max(list(vocab_map.values()))\n",
    "vocab_map['PAD_INDEX'] = 0\n",
    "vocab_map['NOT_FOUND_INDEX'] = max_index+1\n",
    "vocab_size = len(vocab_map)\n",
    "# view vocabulary size and part of the vocabulary map\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Sample slice of vocabulary map:', dict(list(vocab_map.items())[10:20]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM- Encode and Pad datasets & Encode prediction class labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get max length of train corpus and initialize label encoder\n",
    "max_len = np.max([len(review) for review in tokenized_train])\n",
    "\n",
    "## Train reviews data corpus\n",
    "# Convert tokenized text reviews to numeric vectors\n",
    "train_X = [[vocab_map[token] for token in tokenized_review] for tokenized_review in tokenized_train]\n",
    "train_X = sequence.pad_sequences(train_X, maxlen=max_len) # pad \n",
    "\n",
    "## Test reviews data corpus\n",
    "# Convert tokenized text reviews to numeric vectors\n",
    "test_X = [[vocab_map[token] if vocab_map.get(token) else vocab_map['NOT_FOUND_INDEX'] \n",
    "           for token in tokenized_review] \n",
    "              for tokenized_review in tokenized_test]\n",
    "test_X = sequence.pad_sequences(test_X, maxlen=max_len)\n",
    "\n",
    "# view vector shapes\n",
    "print('Max length of train review vectors:', max_len)\n",
    "print('Train review vectors shape:', train_X.shape, ' Test review vectors shape:', test_X.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128 # dimension for dense embeddings for each token\n",
    "LSTM_DIM = 64 # total LSTM units\n",
    "\n",
    "def create_lstm_model(optimizer='adam', init='glorot_uniform'):    \n",
    "    lstm = Sequential()\n",
    "\n",
    "    lstm.add(Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_len))\n",
    "    lstm.add(SpatialDropout1D(0.2))\n",
    "    lstm.add(LSTM(LSTM_DIM, dropout=0.2, recurrent_dropout=0.2))\n",
    "    lstm.add(Dense(2, activation=\"sigmoid\"))\n",
    "    \n",
    "    lstm.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    return lstm\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM- Build, train and visualize the LSTM Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:   \n",
    "    lstm=utils.readFromDisk('lstm')\n",
    "    print(lstm.summary())\n",
    "except FileNotFoundError as te:    \n",
    "    \n",
    "    #see https://machinelearningmastery.com/use-keras-deep-learning-models-scikit-learn-python/\n",
    "    lstm0 = KerasClassifier(build_fn = create_lstm_model)\n",
    "\n",
    "    # grid search epochs, batch size and optimizer\n",
    "    #dnn_optimizers = ['rmsprop', 'adam']\n",
    "    #dnn_init = ['glorot_uniform', 'normal', 'uniform']\n",
    "    lstm_epochs = [1]\n",
    "    lstm_batch_size=[100]\n",
    "    lstm_validation_split=[0.1]\n",
    "    lstm_shuffle=[True]\n",
    "    lstm_workers=[NWORKERS]\n",
    "    lstm_use_multiprocessing=[True]\n",
    "    lstm_verbose=[1]\n",
    "    lstm_hyperparameters = dict(epochs=lstm_epochs, \n",
    "                               batch_size=lstm_batch_size,validation_split=lstm_validation_split,\n",
    "                               shuffle=lstm_shuffle, workers=lstm_workers, use_multiprocessing=lstm_use_multiprocessing,\n",
    "                               verbose=lstm_verbose)\n",
    "    \n",
    "    \n",
    "    lstm_clf = GridSearchCV(lstm0, lstm_hyperparameters,cv=5, n_jobs=1)\n",
    "   \n",
    "    lstm=lstm_clf.fit(train_X, train_sentiments_encoded)    \n",
    "    \n",
    "    #lstm=create_lstm_model()\n",
    "    #lstm.add(Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_len))\n",
    "    #lstm.add(SpatialDropout1D(0.2))\n",
    "    #lstm.add(LSTM(LSTM_DIM, dropout=0.2, recurrent_dropout=0.2))\n",
    "    #lstm.add(Dense(2, activation=\"sigmoid\"))\n",
    "    #lstm.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "    #              metrics=[\"accuracy\"])\n",
    "    #print(lstm.summary())\n",
    "\n",
    "    #SVG(model_to_dot(lstm, show_shapes=True, show_layer_names=False, \n",
    "    #                 rankdir='LR').create(prog='dot', format='svg'))\n",
    "    #batch_size = 100\n",
    "    #lstm.fit(train_X, train_sentiments_encoded, epochs=2, batch_size=batch_size, \n",
    "    #          shuffle=True, validation_split=0.1, verbose=1,workers=NWORKERS,use_multiprocessing=True)\n",
    "    utils.writeToDisk(lstm,'lstm')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lstm.best_estimator_.get_params()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM - Predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lstm_pred_test = lstm.predict(test_X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lstm_predictions = le.inverse_transform(lstm_pred_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM - Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lstm_predictions, \n",
    "                                      classes=[1,0])  \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM - Learning Curve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.learningCurve(train_features,train_sentiments,lstm.best_estimator_,5,np.linspace(.1, 0.7, 7))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ensemble Stacking\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stack_clf1 = KNeighborsClassifier(n_neighbors=1,n_jobs=3)\n",
    "stack_clf2 = RandomForestClassifier(random_state=1,n_jobs=3)\n",
    "stack_clf3 = GaussianNB()\n",
    "stack_lr = LogisticRegression(penalty='l2', max_iter=500, C=60,n_jobs=3)\n",
    "sclf = StackingClassifier(classifiers=[stack_clf1, stack_clf2, stack_clf3], \n",
    "                          meta_classifier=stack_lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensemble Stacking - Predictions via n fold validations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stack_clf_list = [stack_clf1, stack_clf2, stack_clf3, sclf]\n",
    "#preds_list=[]\n",
    "\n",
    "try:\n",
    "    stack_clf_list=utils.readFromDisk('stack_clf_list')\n",
    "    #preds_list=utils.readFromDisk('preds_list')\n",
    "except FileNotFoundError as te:   \n",
    "    stack_clf_list = [stack_clf1, stack_clf2, stack_clf3, sclf]\n",
    "    #stack_preds_list=[]\n",
    "    for stack_clf in stack_clf_list:\n",
    "        stack_clf=stack_clf.fit(train_features,train_sentiments)\n",
    "        #pred = cross_val_predict(clf, train_features, train_sentiments, cv=2,n_jobs=NWORKERS)\n",
    "        #preds_list.append(pred)\n",
    "    \n",
    "    utils.writeToDisk(stack_clf_list,'stack_clf_list')\n",
    "    #utils.writeToDisk(preds_list,'preds_list')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    stack_knn_predictions=utils.readFromDisk('stack_knn_predictions')\n",
    "except FileNotFoundError as te:    \n",
    "    stack_knn_predictions=stack_clf1.predict(test_features)\n",
    "    utils.writeToDisk(stack_knn_predictions,'stack_knn_predictions')\n",
    "try:\n",
    "    stack_rf_predictions=utils.readFromDisk('stack_rf_predictions')\n",
    "except FileNotFoundError as te:    \n",
    "    stack_rf_predictions=stack_clf2.predict(test_features)\n",
    "    utils.writeToDisk(stack_rf_predictions,'stack_rf_predictions')\n",
    "\n",
    "try:\n",
    "    stack_gau_predictions=utils.readFromDisk('stack_gau_predictions')\n",
    "except FileNotFoundError as te:    \n",
    "    stack_gau_predictions=stack_clf3.predict(test_features)\n",
    "    utils.writeToDisk(stack_gau_predictions,'stack_gau_predictions')\n",
    "\n",
    "try:\n",
    "    stack_stack_predictions=utils.readFromDisk('stack_predictions')\n",
    "except FileNotFoundError as te:    \n",
    "    stack_predictions=sclf.predict(test_features)\n",
    "    utils.writeToDisk(stack_predictions,'stack_predictions')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensemble Stacking Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=stack_knn_predictions, \n",
    "                                      classes=[1,0])  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=stack_rf_predictions, \n",
    "                                      classes=[1,0])  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=stack_gau_predictions, \n",
    "                                      classes=[1,0])  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=stack_predictions, \n",
    "                                      classes=[1,0])  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ensemble Majority Vote\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MajorityVoteClassifier(BaseEstimator, \n",
    "                             ClassifierMixin):\n",
    "    \"\"\" A majority vote ensemble classifier\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    classifiers : array-like, shape = [n_classifiers]\n",
    "      Different classifiers for the ensemble\n",
    "\n",
    "    vote : str, {'classlabel', 'probability'} (default='label')\n",
    "      If 'classlabel' the prediction is based on the argmax of\n",
    "        class labels. Else if 'probability', the argmax of\n",
    "        the sum of probabilities is used to predict the class label\n",
    "        (recommended for calibrated classifiers).\n",
    "\n",
    "    weights : array-like, shape = [n_classifiers], optional (default=None)\n",
    "      If a list of `int` or `float` values are provided, the classifiers\n",
    "      are weighted by importance; Uses uniform weights if `weights=None`.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, classifiers, vote='classlabel', weights=None):\n",
    "\n",
    "        self.classifiers = classifiers\n",
    "        self.named_classifiers = {key: value for key, value\n",
    "                                  in _name_estimators(classifiers)}\n",
    "        self.vote = vote\n",
    "        self.weights = weights\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit classifiers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Matrix of training samples.\n",
    "\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Vector of target class labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "        if self.vote not in ('probability', 'classlabel'):\n",
    "            raise ValueError(\"vote must be 'probability' or 'classlabel'\"\n",
    "                             \"; got (vote=%r)\"\n",
    "                             % self.vote)\n",
    "\n",
    "        if self.weights and len(self.weights) != len(self.classifiers):\n",
    "            raise ValueError('Number of classifiers and weights must be equal'\n",
    "                             '; got %d weights, %d classifiers'\n",
    "                             % (len(self.weights), len(self.classifiers)))\n",
    "\n",
    "        # Use LabelEncoder to ensure class labels start with 0, which\n",
    "        # is important for np.argmax call in self.predict\n",
    "        self.lablenc_ = LabelEncoder()\n",
    "        self.lablenc_.fit(y)\n",
    "        self.classes_ = self.lablenc_.classes_\n",
    "        self.classifiers_ = []\n",
    "        for clf in self.classifiers:\n",
    "            fitted_clf = clone(clf).fit(X, self.lablenc_.transform(y))\n",
    "            self.classifiers_.append(fitted_clf)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict class labels for X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Matrix of training samples.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        maj_vote : array-like, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "            \n",
    "        \"\"\"\n",
    "        if self.vote == 'probability':\n",
    "            maj_vote = np.argmax(self.predict_proba(X), axis=1)\n",
    "        else:  # 'classlabel' vote\n",
    "\n",
    "            #  Collect results from clf.predict calls\n",
    "            predictions = np.asarray([clf.predict(X)\n",
    "                                      for clf in self.classifiers_]).T\n",
    "\n",
    "            maj_vote = np.apply_along_axis(\n",
    "                                      lambda x:\n",
    "                                      np.argmax(np.bincount(x,\n",
    "                                                weights=self.weights)),\n",
    "                                      axis=1,\n",
    "                                      arr=predictions)\n",
    "        maj_vote = self.lablenc_.inverse_transform(maj_vote)\n",
    "        return maj_vote\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\" Predict class probabilities for X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        avg_proba : array-like, shape = [n_samples, n_classes]\n",
    "            Weighted average probability for each class per sample.\n",
    "\n",
    "        \"\"\"\n",
    "        probas = np.asarray([clf.predict_proba(X)\n",
    "                             for clf in self.classifiers_])\n",
    "        avg_proba = np.average(probas, axis=0, weights=self.weights)\n",
    "        return avg_proba\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\" Get classifier parameter names for GridSearch\"\"\"\n",
    "        if not deep:\n",
    "            return super(MajorityVoteClassifier, self).get_params(deep=False)\n",
    "        else:\n",
    "            out = self.named_classifiers.copy()\n",
    "            for name, step in six.iteritems(self.named_classifiers):\n",
    "                for key, value in six.iteritems(step.get_params(deep=True)):\n",
    "                    out['%s__%s' % (name, key)] = value\n",
    "            return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "maj_clf1 = LogisticRegression(penalty='l2', C=0.1, max_iter=500, random_state=0,n_jobs=3)\n",
    "maj_clf2 = DecisionTreeClassifier(max_depth=1, criterion='entropy', random_state=0)\n",
    "maj_clf3 = KNeighborsClassifier(n_neighbors=1,p=2, metric='minkowski',n_jobs=3)\n",
    "maj_pipe1 = Pipeline([['sc', StandardScaler()],\n",
    "                  ['clf', maj_clf1]])\n",
    "maj_pipe3 = Pipeline([['sc', StandardScaler()],\n",
    "                  ['clf', maj_clf3]])\n",
    "maj_clf_labels = ['Logistic Regression', 'Decision Tree', 'KNN','Majority Voting']\n",
    "mclf = MajorityVoteClassifier(classifiers=[maj_pipe1, maj_clf2, maj_pipe3])\n",
    "maj_clf_list = [maj_pipe1, maj_clf2, maj_pipe3, mclf]\n",
    "\n",
    "\n",
    "try:\n",
    "    maj_clf_list=utils.readFromDisk('maj_clf_list')\n",
    "except FileNotFoundError as te:   \n",
    "    maj_clf_list = [maj_pipe1, maj_clf2, maj_pipe3, mclf]\n",
    "    for maj_clf in maj_clf_list:\n",
    "        maj_clf=maj_clf.fit(train_features,train_sentiments)\n",
    "    \n",
    "    utils.writeToDisk(maj_clf_list,'maj_clf_list')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    maj_lr_predictions=utils.readFromDisk('maj_lr_predictions')\n",
    "except FileNotFoundError as te:    \n",
    "    maj_lr_predictions=maj_clf1.predict(test_features)\n",
    "    utils.writeToDisk(maj_lr_predictions,'maj_lr_predictions')\n",
    "try:\n",
    "    maj_dt_predictions=utils.readFromDisk('maj_dt_predictions')\n",
    "except FileNotFoundError as te:    \n",
    "    maj_dt_predictions=maj_clf2.predict(test_features)\n",
    "    utils.writeToDisk(maj_dt_predictions,'maj_dt_predictions')\n",
    "\n",
    "try:\n",
    "    maj_knn_predictions=utils.readFromDisk('maj_knn_predictions')\n",
    "except FileNotFoundError as te:    \n",
    "    maj_knn_predictions=maj_clf3.predict(test_features)\n",
    "    utils.writeToDisk(maj_knn_predictions,'maj_knn_predictions')\n",
    "\n",
    "\n",
    "try:\n",
    "    maj_predictions=utils.readFromDisk('maj_predictions')\n",
    "except FileNotFoundError as te:    \n",
    "    maj_predictions=mclf.predict(test_features)\n",
    "    utils.writeToDisk(maj_predictions,'maj_predictions')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensemble Majority Voting Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=maj_lr_predictions,\n",
    "                                      classes=[1,0])  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=maj_dt_predictions, \n",
    "                                      classes=[1,0])  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=maj_knn_predictions, \n",
    "                                      classes=[1,0])  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=maj_predictions, \n",
    "                                      classes=[1,0])  \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# All Models Evaluation\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model ROC curves"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(0).clf()\n",
    "\n",
    "color = ['blue', 'orange', 'red', 'green', 'coral',\n",
    "             'grey', 'indigo', 'gold', 'lime', 'olive',\n",
    "             'pink', 'navy', 'magenta', 'yellow', 'tomato',\n",
    "             'turquoise', 'yellowgreen', 'maroon', 'lightblue']\n",
    "mlr=[]\n",
    "msvm=[]\n",
    "mdnn=[]\n",
    "mxg=[]\n",
    "mlstm=[]\n",
    "mknn=[]\n",
    "mrf=[]\n",
    "mgau=[]\n",
    "mstack=[]\n",
    "\n",
    "def metricsAndROC(pred,metricsArray,rocTitle,colorIndex):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_sentiments, pred)\n",
    "    auc = metrics.roc_auc_score(test_sentiments, pred)\n",
    "    metricsArray.append(metrics.f1_score(test_sentiments, pred))\n",
    "    metricsArray.append(metrics.precision_score(test_sentiments, pred))\n",
    "    metricsArray.append(metrics.accuracy_score(test_sentiments, pred))\n",
    "    metricsArray.append(metrics.recall_score(test_sentiments, pred))\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr,color=color[colorIndex], label=rocTitle.format(auc))\n",
    "\n",
    "\n",
    "\n",
    "metricsAndROC(lr_predictions,mlr,'LR',0)\n",
    "metricsAndROC(svm_predictions,msvm,'SVM',1)\n",
    "metricsAndROC(dnn_predictions,mdnn,'DNN',2)\n",
    "metricsAndROC(xg_predictions,mxg,'XGBoost',3)\n",
    "metricsAndROC(lstm_predictions,mlstm,'LSTM',4)\n",
    "metricsAndROC(stack_knn_predictions,mknn,'KNN',5)\n",
    "metricsAndROC(stack_rf_predictions,mrf,'RFOR',6)\n",
    "metricsAndROC(stack_gau_predictions,mgau,'GAUS',7)\n",
    "metricsAndROC(stack_predictions,mstack,'ENSEM-STACK',8)\n",
    "\n",
    "\n",
    "\n",
    "#show the roc curve now\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "# show the legend\n",
    "plt.legend(loc='center right')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## All Models Metrics comparison"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_groups = 4\n",
    "index = np.arange(n_groups)\n",
    "bar_width = .1\n",
    "\n",
    "plt.bar(index,mlr, bar_width, color=color[0], label='LR')\n",
    "\n",
    "\n",
    "z=index + bar_width\n",
    "plt.bar(z, msvm, bar_width, color=color[1],label='SVM')\n",
    "\n",
    "\n",
    "z=z+ bar_width\n",
    "plt.bar(z, mdnn, bar_width, color=color[2], label='DNN')\n",
    "\n",
    "z=z+ bar_width\n",
    "plt.bar(z,mxg , bar_width, color=color[3], label='XGB')\n",
    "\n",
    "z=z+ bar_width\n",
    "plt.bar(z,mlstm , bar_width,color=color[4], label='LSTM')\n",
    "\n",
    "\n",
    "\n",
    "#ax.set_xlabel('Metric')\n",
    "#ax.set_ylabel('Value')\n",
    "#ax.set_title('Comparison of Feature Engineering Models on Amazon Reviews')\n",
    "#ax.set_xticks(index + bar_width / 2)\n",
    "pltLabels=['F1','PRECISION','ACCURACY','RECALL']\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('Model Metrics', fontweight='bold')\n",
    "plt.xticks([r + bar_width for r in range(n_groups)], pltLabels)\n",
    " \n",
    "# Create legend & Show graphic\n",
    "plt.legend(frameon=False,ncol=3, loc='lower left')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}