{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "NUMFEATURES=512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  not bother think would see movie great supspen...  negative\n",
      "1  careful one get mitt change way look kung fu f...  positive\n",
      "2  chili palmer tired movie know want success mus...  negative\n",
      "3  follow little know 1998 british film make budg...  positive\n",
      "4  dark angel cross huxley brave new world percys...  positive\n"
     ]
    }
   ],
   "source": [
    "# build train and test datasets\n",
    "train_reviews = utils.readFromDisk('train_reviews')\n",
    "train_sentiments = utils.readFromDisk('train_sentiments')\n",
    "test_reviews = utils.readFromDisk('test_reviews')\n",
    "test_sentiments = utils.readFromDisk('test_sentiments')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Text Classification Pipeline with The Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Build Text Classification Pipeline\n",
    "model = utils.readFromDisk('svm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analyze Model Prediction Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=['the lord of the rings is an excellent movie'];\n",
    "model.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.162039</td>\n",
       "      <td>0.837961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.745614</td>\n",
       "      <td>0.254386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   negative  positive\n",
       "0  0.162039  0.837961\n",
       "1  0.745614  0.254386"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model.predict_proba(['the lord of the rings is an excellent movie', \n",
    "                     'i hated the recent movie on tv, it was so bad']), columns=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Model Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skater'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e45e5360a2bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mskater\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskater\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_interpretation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlime_text\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLimeTextExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#https://www.oreilly.com/ideas/interpreting-predictive-models-with-skater-unboxing-model-opacity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLimeTextExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skater'"
     ]
    }
   ],
   "source": [
    "import skater\n",
    "from skater.core.local_interpretation.lime.lime_text import LimeTextExplainer\n",
    "#https://www.oreilly.com/ideas/interpreting-predictive-models-with-skater-unboxing-model-opacity\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=classes)\n",
    "def interpret_classification_model_prediction(doc_index, norm_corpus, corpus, \n",
    "                                              prediction_labels, explainer_obj):\n",
    "    # display model prediction and actual sentiments\n",
    "    print(\"Test document index: {index}\\nActual sentiment: {actual}\\nPredicted sentiment: {predicted}\"\n",
    "      .format(index=doc_index, actual=prediction_labels[doc_index],\n",
    "              predicted=model.predict([norm_corpus[doc_index]])))\n",
    "    # display actual review content\n",
    "    print(\"\\nReview:\", corpus[doc_index])\n",
    "    # display prediction probabilities\n",
    "    print(\"\\nModel Prediction Probabilities:\")\n",
    "    for probs in zip(classes, model.predict_proba([norm_corpus[doc_index]])[0]):\n",
    "        print(probs)\n",
    "    # display model prediction interpretation\n",
    "    exp = explainer.explain_instance(norm_corpus[doc_index], \n",
    "                                     model.predict_proba, num_features=10, \n",
    "                                     labels=[1])\n",
    "    exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interpret_classification_model_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d0c7e0dc64a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdoc_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m interpret_classification_model_prediction(doc_index=doc_index, norm_corpus=test_reviews,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                          \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_sentiments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                          explainer_obj=explainer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'interpret_classification_model_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "doc_index = 100 \n",
    "interpret_classification_model_prediction(doc_index=doc_index, norm_corpus=test_reviews,\n",
    "                                         corpus=test_reviews, prediction_labels=test_sentiments,\n",
    "                                         explainer_obj=explainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interpret_classification_model_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e543a376ddde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdoc_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m interpret_classification_model_prediction(doc_index=doc_index, norm_corpus=norm_test_reviews,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                          \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_sentiments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                          explainer_obj=explainer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'interpret_classification_model_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "doc_index = 2000\n",
    "interpret_classification_model_prediction(doc_index=doc_index, norm_corpus=test_reviews,\n",
    "                                         corpus=test_reviews, prediction_labels=test_sentiments,\n",
    "                                         explainer_obj=explainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interpret_classification_model_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1cd175b2566a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdoc_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m347\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m interpret_classification_model_prediction(doc_index=doc_index, norm_corpus=norm_test_reviews,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                          \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_sentiments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                          explainer_obj=explainer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'interpret_classification_model_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "doc_index = 347 \n",
    "interpret_classification_model_prediction(doc_index=doc_index, norm_corpus=test_reviews,\n",
    "                                         corpus=test_reviews, prediction_labels=test_sentiments,\n",
    "                                         explainer_obj=explainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}