{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis for Cellphone and Accessories category on Amazon \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\rkaushik\\AppData\\Roaming\\nltk_data...",
      "\n",
      "[nltk_data]   Package wordnet is already up-to-date!",
      "\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 6
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import model_evaluation_utils as meu\n",
    "import utils\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "import warnings\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import multiprocessing\n",
    "from multiprocessing import Process\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "\n",
    "nlp = spacy.load('en_vecs', parse=False, tag=False, entity=False)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "PROCESSED_FILENAME= './data/amazon_reviews_processed.pickle' \n",
    "NWORKERS=16\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load normalized data from processed file\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f=open(PROCESSED_FILENAME, \"rb\")\n",
    "dfdb = pickle.load(f)\n",
    "\n",
    "#filter rows out that have less than 20 word tokens\n",
    "dfdb = dfdb[dfdb['Clean_Review_Tokens'].apply(lambda x: len(x) >= 20)]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prune data for development if needed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trial=0\n",
    "\n",
    "#subset for local runs, will remove on final runs or on server\n",
    "five=(dfdb['overall'] == 5.0)\n",
    "four=(dfdb['overall'] >= 4.0) & (dfdb['overall'] < 5.0)\n",
    "three=(dfdb['overall'] == 3.0) & (dfdb['overall'] < 4.0)\n",
    "two=(dfdb['overall'] == 2.0) & (dfdb['overall'] < 3.0)\n",
    "one=(dfdb['overall'] == 1.0) & (dfdb['overall'] < 2.0)\n",
    "zero=(dfdb['overall'] == 0.0) & (dfdb['overall'] < 1.0)\n",
    "\n",
    "df=pd.DataFrame(columns = dfdb.columns)\n",
    "if(trial>0):\n",
    "    df=dfdb[five].iloc[0:trial]\n",
    "    df=df.append(dfdb[four].iloc[0:trial])\n",
    "    df=df.append(dfdb[two].iloc[0:trial])\n",
    "    df=df.append(dfdb[one].iloc[0:trial])\n",
    "    df=df.append(dfdb[zero].iloc[0:trial])\n",
    "else:\n",
    "    df=dfdb[five]\n",
    "    df=df.append(dfdb[four])\n",
    "    df=df.append(dfdb[two])\n",
    "    df=df.append(dfdb[one])\n",
    "    df=df.append(dfdb[zero])\n",
    "\n",
    "#randomize dataset\n",
    "df = shuffle(df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample processed data loaded, notice Cleaned Review"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Total Rows on processed dataset: ' + str(len(df)))\n",
    "print('Sample of processed dataset. Notice the column named Clean_Review');\n",
    "df.head(20)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split train and test data\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# take a peek at the data\n",
    "reviews = np.array(df['Clean_Review'])\n",
    "sentiments = np.array(df['sentiment'])\n",
    "reviews_tokens = np.array(df['Clean_Review_Tokens'])\n",
    "\n",
    "cutoff=round(len(df)*0.75)\n",
    "# build train and test datasets\n",
    "train_reviews = reviews[:cutoff]\n",
    "train_reviews_tokens = reviews_tokens[:cutoff]\n",
    "\n",
    "train_sentiments = sentiments[:cutoff]\n",
    "train_sentiments=train_sentiments.astype('int')\n",
    "\n",
    "test_reviews = reviews[cutoff:]\n",
    "test_reviews_tokens = reviews_tokens[cutoff:]\n",
    "\n",
    "test_sentiments = sentiments[cutoff:]\n",
    "test_sentiments=test_sentiments.astype('int')\n",
    "\n",
    "#sample_review_ids = [1000, 5000, 10000,15000,20000,25000,30000,35000,40000,45000,50000,60000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample train data loaded, notice Cleaned Review"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Total Rows on train dataset: ' + str(len(train_reviews)))\n",
    "print('Total Rows on test dataset: ' + str(len(test_reviews)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis with AFINN\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "afn = Afinn(emoticons=True) \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predict sentiment by AFINN for test dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "sentiment_polarity = [afn.score(review) for review in test_reviews]\n",
    "predicted_sentiments_afinn = [1 if score >= 1.0 else 0 for score in sentiment_polarity]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate model performance of AFINN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model Performance metrics:",
      "\n",
      "------------------------------",
      "\n",
      "Accuracy:",
      " ",
      "0.7054",
      "\n",
      "Precision:",
      " ",
      "0.7212",
      "\n",
      "Recall:",
      " ",
      "0.7054",
      "\n",
      "F1 Score:",
      " ",
      "0.6993",
      "\n",
      "\nModel Classification report:",
      "\n",
      "------------------------------",
      "\n",
      "              precision    recall  f1-score   support\n\n    positive       0.66      0.84      0.74      7587\n    negative       0.78      0.56      0.65      7413\n\n    accuracy                           0.71     15000\n   macro avg       0.72      0.70      0.70     15000\nweighted avg       0.72      0.71      0.70     15000\n",
      "\n",
      "\nPrediction Confusion Matrix:",
      "\n",
      "------------------------------",
      "\n",
      "                 Predicted:         \n                   positive negative\nActual: positive       6405     1182\n        negative       3237     4176",
      "\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\rkaushik\\PycharmProjects\\ML1010_InClass\\Day1\\3_sentiment\\model_evaluation_utils.py:60: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n  labels=level_labels),\n",
      "C:\\Users\\rkaushik\\PycharmProjects\\ML1010_InClass\\Day1\\3_sentiment\\model_evaluation_utils.py:62: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n  labels=level_labels))\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments_afinn, \n",
    "                                  classes=[1, 0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis with SentiWordNet\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build SentiWordnet model (multiprocessing)\n",
    "## Predict SentiWordnet sentiment for test dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#predicted_sentiments = [analyze_sentiment_sentiwordnet_lexicon(review, verbose=False) for review in test_reviews]\n",
    "\n",
    "#number of observations\n",
    "size=len(test_reviews)\n",
    "#number of observations in each process\n",
    "iterSize=round(size/NWORKERS)\n",
    "#holds the processes\n",
    "processes=[]\n",
    "\n",
    "print('To process: ' + str(size) + ' across '+ str(NWORKERS) +' workers ')\n",
    " \n",
    "i=0\n",
    "\n",
    "predicted_sentiments_sn=[]\n",
    "sentiments_holder=[None]*NWORKERS\n",
    "parent_conn_holder=[]\n",
    "\n",
    "for i in range(0,NWORKERS):\n",
    "    start=i*iterSize\n",
    "    stop=start+iterSize\n",
    "    if(i==(NWORKERS-1)):\n",
    "        stop=size\n",
    "    #split df for parallel proc\n",
    "    reviews_proc=test_reviews[start:stop]\n",
    "    # creating a pipe \n",
    "    parent_conn, child_conn = multiprocessing.Pipe() \n",
    "    p = Process(target=utils.analyze_sentiment_sentiwordnet_lexicon_multiproc, args=(reviews_proc,child_conn,i))\n",
    "    processes.append(p)\n",
    "    p.start()\n",
    "    parent_conn_holder.append(parent_conn)\n",
    "\n",
    "    \n",
    "for parent_conn in parent_conn_holder:\n",
    "    review_iter=parent_conn.recv()\n",
    "    sentiments_holder[review_iter[0]]=review_iter[1:]\n",
    "\n",
    "for p in processes:\n",
    "    p.join()\n",
    "\n",
    "for item in sentiments_holder:\n",
    "    for review in item:\n",
    "        predicted_sentiments_sn.append(review)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate Sentiwordnet model performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model Performance metrics:",
      "\n",
      "------------------------------",
      "\n",
      "Accuracy:",
      " ",
      "0.6776",
      "\n",
      "Precision:",
      " ",
      "0.6804",
      "\n",
      "Recall:",
      " ",
      "0.6776",
      "\n",
      "F1 Score:",
      " ",
      "0.6758",
      "\n",
      "\nModel Classification report:",
      "\n",
      "------------------------------",
      "\n",
      "              precision    recall  f1-score   support\n\n    positive       0.66      0.75      0.70      7587\n    negative       0.70      0.61      0.65      7413\n\n    accuracy                           0.68     15000\n   macro avg       0.68      0.68      0.68     15000\nweighted avg       0.68      0.68      0.68     15000\n",
      "\n",
      "\nPrediction Confusion Matrix:",
      "\n",
      "------------------------------",
      "\n",
      "                 Predicted:         \n                   positive negative\nActual: positive       5679     1908\n        negative       2928     4485",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments_sn, \n",
    "                                  classes=[1, 0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis with VADER\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Vader model (multiprocessing)\n",
    "## Predict Vader sentiment for test dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "\n",
    "#predicted_sentiments = [analyze_sentiment_vader_lexicon(review, threshold=0.4, verbose=False) for review in test_reviews]\n",
    "\n",
    "#number of observations\n",
    "size=len(test_reviews)\n",
    "#number of observations in each process\n",
    "iterSize=round(size/NWORKERS)\n",
    "#holds the processes\n",
    "processes=[]\n",
    "\n",
    "print('To process: ' + str(size) + ' across '+ str(NWORKERS) +' workers ')\n",
    " \n",
    "i=0\n",
    "\n",
    "predicted_sentiments_vader=[]\n",
    "sentiments_holder=[None]*NWORKERS\n",
    "parent_conn_holder=[]\n",
    "\n",
    "for i in range(0,NWORKERS):\n",
    "    start=i*iterSize\n",
    "    stop=start+iterSize\n",
    "    if(i==(NWORKERS-1)):\n",
    "        stop=size\n",
    "    #split df for parallel proc\n",
    "    reviews_proc=test_reviews[start:stop]\n",
    "    # creating a pipe \n",
    "    parent_conn, child_conn = multiprocessing.Pipe() \n",
    "    p = Process(target=utils.analyze_sentiment_vader_multiproc, args=(reviews_proc,0.4,child_conn,i))\n",
    "    processes.append(p)\n",
    "    p.start()\n",
    "    parent_conn_holder.append(parent_conn)\n",
    "\n",
    "    \n",
    "for parent_conn in parent_conn_holder:\n",
    "    review_iter=parent_conn.recv()\n",
    "    sentiments_holder[review_iter[0]]=review_iter[1:]\n",
    "\n",
    "for p in processes:\n",
    "    p.join()\n",
    "\n",
    "for item in sentiments_holder:\n",
    "    for review in item:\n",
    "        predicted_sentiments_vader.append(review)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate Vader model performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model Performance metrics:",
      "\n",
      "------------------------------",
      "\n",
      "Accuracy:",
      " ",
      "0.6964",
      "\n",
      "Precision:",
      " ",
      "0.704",
      "\n",
      "Recall:",
      " ",
      "0.6964",
      "\n",
      "F1 Score:",
      " ",
      "0.6929",
      "\n",
      "\nModel Classification report:",
      "\n",
      "------------------------------",
      "\n",
      "              precision    recall  f1-score   support\n\n    positive       0.67      0.80      0.73      7587\n    negative       0.74      0.59      0.66      7413\n\n    accuracy                           0.70     15000\n   macro avg       0.70      0.70      0.69     15000\nweighted avg       0.70      0.70      0.69     15000\n",
      "\n",
      "\nPrediction Confusion Matrix:",
      "\n",
      "------------------------------",
      "\n",
      "                 Predicted:         \n                   positive negative\nActual: positive       6066     1521\n        negative       3033     4380",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments_vader, \n",
    "                                  classes=[1, 0])\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}